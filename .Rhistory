ReturnDate = max(ReturnDate, na.rm = TRUE)
)
filtered_house_travel <- house_travel %>%
anti_join(duplicate_docids_house_travel, by = "DocID")
# Identify duplicate rows based on the combination of values in specified columns that remain consistent from form to form.
# To cast a wide net, we create one version that identifies rows with matching values in the FilerName, MemberName, State, Year and standardized_dest columns -- columns that consistently match from entry to entry.
columns_wide <- c("FilerName", "MemberName", "State", "Year", "standardized_dest")
# To cast a narrower net, we create another version that identifies rows with matching values in the FilerName, MemberName, State, and standardized_dest columns, as well as the year_month and sponsors_cleaned columns -- columns that may change within an otherwise matching set.
columns_narrow <- c("FilerName", "MemberName", "State", "year_month", "standardized_dest", "sponsors_cleaned")
duplicates_logical_wide <- duplicated(filtered_house_travel[columns_wide], fromLast = TRUE) | duplicated(filtered_house_travel[columns_wide])
duplicates_logical_narrow <- duplicated(filtered_house_travel[columns_narrow], fromLast = TRUE) | duplicated(filtered_house_travel[columns_narrow])
# Subset the original dataset using the wide and narrow duplicate vectors to get the duplicate rows
duplicates_wide <- subset(filtered_house_travel, duplicates_logical_wide)
duplicates_narrow <- subset(filtered_house_travel, duplicates_logical_narrow)
# Arrange the rows to consolidate matching groups.
duplicates_wide <- arrange(duplicates_wide, FilerName, MemberName, State, Year, standardized_dest)
duplicates_narrow <- arrange(duplicates_narrow, FilerName, MemberName, State, Year, standardized_dest, sponsors_cleaned)
missing <- anti_join(duplicates_narrow, duplicates_wide, by = c("DocID"))
# Use the DocID column to identify the most recent version of a travel filing.
rows_to_keep <- duplicates_narrow %>%
group_by(FilerName, MemberName, State, year_month, standardized_dest, sponsors_cleaned) %>%
filter(DocID == max(DocID))
diff_sponsor_month <- anti_join(duplicates_wide, duplicates_narrow, by = c("FilerName", "MemberName", "State", "year_month", "standardized_dest", "sponsors_cleaned"))
no_change_diff_sponsor_month_rows <- diff_sponsor_month %>%
group_by(across(-c(TravelSponsor, sponsors_cleaned, DocID, index_number, FilingType, Destination))) %>%
filter(n() == 1) %>%
ungroup()
consolidated_diff_sponsor_rows <- diff_sponsor_month %>%
group_by(across(-c(TravelSponsor, sponsors_cleaned, DocID, index_number, FilingType, Destination))) %>%
filter(n() > 1) %>%
slice_max(order_by = DocID) %>%
ungroup()
filtered_house_travel <- anti_join(filtered_house_travel, duplicates_wide, by = c("DocID"))
# Convert 'index_number' to numeric in combined_duplicate_docids
combined_duplicate_docids <- mutate(combined_duplicate_docids, index_number = as.numeric(index_number))
# Now you can bind the rows
filtered_house_travel <- bind_rows(filtered_house_travel, rows_to_keep)
filtered_house_travel <- bind_rows(filtered_house_travel, combined_duplicate_docids)
filtered_house_travel <- bind_rows(filtered_house_travel, consolidated_diff_sponsor_rows)
# Group by the specified columns and count the occurrences of each combination
grouped_same_dest_diff_spons <- no_change_diff_sponsor_month_rows %>%
group_by(MemberName, FilerName, State, standardized_dest, year_month) %>%
summarise(count = n()) %>%
filter(count > 1)
# Filter for rows where count is greater than 1, meaning they have duplicates
same_dest_diff_spons <- no_change_diff_sponsor_month_rows %>%
inner_join(grouped_same_dest_diff_spons, by = c('MemberName', 'FilerName', 'State', 'standardized_dest', 'year_month'))
# Group by the specified columns and count the occurrences of each combination
grouped_same_dest_diff_month <- no_change_diff_sponsor_month_rows %>%
group_by(MemberName, FilerName, State, standardized_dest, sponsors_cleaned) %>%
summarise(count = n()) %>%
filter(count > 1)
# Filter for rows where count is greater than 1, meaning they have duplicates
same_dest_diff_month <- no_change_diff_sponsor_month_rows %>%
inner_join(grouped_same_dest_diff_month, by = c('MemberName', 'FilerName', 'State', 'standardized_dest', 'sponsors_cleaned'))
no_change_diff_sponsor_month_rows <- subset(no_change_diff_sponsor_month_rows,
!(MemberName == "Beyer, Donald" &
FilingType == "Original" &
DepartureDate == "2021-11-19" &
sponsors_cleaned == "United Kingdom Marshall Scholars"))
possible_duplicates <- subset(no_change_diff_sponsor_month_rows,
sponsors_cleaned %in% c("Us-qatar Business Council",
"Hillsdale College",
"Washington Office On Latin America",
"Japan Center For International Exchange"))
filtered_no_change_diff_sponsor_month_rows <- anti_join(no_change_diff_sponsor_month_rows, possible_duplicates, by = c("DocID"))
filtered_house_travel <- bind_rows(filtered_house_travel, filtered_no_change_diff_sponsor_month_rows)
# Write the filtered house travel dataset to a CSV file
write_csv(filtered_house_travel, "deduped_house_travel_full.csv")
# Write the remaining possible duplicates dataset to a CSV file.
write_csv(possible_duplicates, "possible_duplicates.csv")
filtered_house_travel %>%
group_by(sponsors_cleaned) %>%
summarise(count = n()) %>%
arrange(desc(count))
biofuels_filtered <- filtered_house_travel[grepl("Renewable Fuels|Clean Fuels|Biodiesel|Biofuel", filtered_house_travel$sponsors_cleaned, ignore.case = TRUE), ]
biofuels_filtered |>
filter(Year > 2020) |>
arrange(desc(DepartureDate))
sugar_filtered <- filtered_house_travel[grepl("Sugar|Sugarbeet|Sugarcane|South Florida Agricultural|Agricultural Institute of Florida|Louisiana Farm Bureau|Leadership Idaho Agriculture", filtered_house_travel$sponsors_cleaned, ignore.case = TRUE), ]
sugar_filtered |>
arrange(desc(DepartureDate))
sugar_filtered |>
group_by(sponsors_cleaned) |>
summarize(number_trips = n()) |>
arrange(desc(number_trips))
sugar_travelers <- sugar_filtered |>
group_by(FilerName) |>
summarize(number_trips = n()) |>
arrange(desc(number_trips))
sugar_filtered |>
group_by(MemberName) |>
filter(Year <= 2018) |>
summarize(number_trips = n()) |>
arrange(desc(number_trips))
sugar_red_river <- sugar_filtered[str_detect(sugar_filtered$sponsors_cleaned, "Red River"), ]
sugar_red_river |>
group_by(Year)|>
summarise(count = n()) |>
arrange(desc(count))
sugar_filtered |>
group_by(State) |>
summarize(number_trips = n()) |>
arrange(desc(number_trips))
# Convert the "Date" column to Date type if it's not already
sugar_filtered$Date <- as.Date(sugar_filtered$DepartureDate)
# Create a line chart
sugar_filtered_plot <- ggplot(sugar_filtered, aes(x = DepartureDate)) +
geom_line(stat = "count", color = "blue") +
labs(title = "Number of Trips Sponsored in Whole or in Part by Sugar Interests",
x = "Date",
y = "Trips") +
theme_minimal()
sugar_filtered_plot
# Convert the "Date" column to Date type if it's not already
sugar_red_river$Date <- as.Date(sugar_red_river$DepartureDate)
# Create a line chart
sugar_red_river_plot <- ggplot(sugar_red_river, aes(x = DepartureDate)) +
geom_line(stat = "count", color = "blue") +
labs(title = "Number of Trips Sponsored in Whole or in Part by Sugar Interests",
x = "Date",
y = "Trips") +
theme_minimal()
sugar_red_river_plot
sugar_filtered <- sugar_filtered %>%
mutate(Month = format(DepartureDate, "%Y-%m"))
sugar_dests_by_month_filtered <- sugar_filtered %>%
group_by(Month, standardized_dest) %>%
summarise(count = n()) %>%
arrange(desc(count))
top5_per_month_sugar_filtered <- sugar_dests_by_month_filtered %>%
group_by(Month) %>%
top_n(5, count)
top5_per_month_sugar_filtered |>
arrange(desc(Month))
# Filter rows to isolate trips sponsored in whole or in part by other agricultural interests.
other_ag <- filtered_house_travel[grepl("St. Louis Agribusiness Club|Farm Journal Foundation|National Association of State Departments of Agriculture|Farm Credit of the Virginias|Bowery Farming|Massachusetts Farm Bureau|National Farmers Union|National Grain and Feed Association|North Carolina Vegetable Growers Association|Livestock Marketing Association|Farm Foundation", filtered_house_travel$sponsors_cleaned, ignore.case = TRUE), ]
other_ag|>
arrange(desc(DepartureDate))
# Filter rows to isolate trips sponsored by conservation or clean energy organizations that may have an interest in the sugar industry or its environmental impacts.
conservation_sugar_regions_narrow <- house_travel[grepl("The Everglades Foundation|National Parks Conservation Association|Center for Clean Air Policy|Coalition to Restore Coastal Louisiana|Idaho Conservation League", house_travel$sponsors_cleaned, ignore.case = TRUE), ]
conservation_sugar_regions_narrow|>
group_by(MemberName) |>
summarize(count = n()) |>
arrange(desc(count))
print(conservation_sugar_regions_narrow)
# Filter rows to isolate trips sponsored by conservation or clean energy organizations that may have an interest in the sugar industry or its environmental impacts.
everglades_foundation <- house_travel[grepl("The Everglades Foundation", house_travel$sponsors_cleaned, ignore.case = TRUE), ]
everglades_foundation|>
group_by(MemberName) |>
summarize(count = n()) |>
arrange(desc(count))
print(everglades_foundation)
everglades_foundation$Date <- as.Date(everglades_foundation$DepartureDate)
# Create a line chart
everglades_foundation_plot <- ggplot(everglades_foundation, aes(x = DepartureDate)) +
geom_line(stat = "count", color = "blue") +
labs(title = "Number of Trips Sponsored by the Everglades Foundation",
x = "Date",
y = "Trips") +
theme_minimal()
everglades_foundation_plot
louisiana_sugar <- subset(sugar_filtered, grepl("Louisiana Sugar", TravelSponsor, ignore.case = TRUE))
# Display the resulting subset
print(louisiana_sugar)
louisiana_sugar |>
group_by(Year) |>
summarize(trip_count = n()) |>
arrange(desc(trip_count))
louisiana_sugar |>
filter(Year>2014 & Year<2018) |>
group_by(MemberName, Year) |>
summarize(trip_count = n()) |>
arrange(desc(Year))
louisiana_sugar |>
group_by(State) |>
summarize(trip_count = n()) |>
arrange(desc(trip_count))
midwest_sugar <- sugar_filtered[grepl("Sugarbeet|Leadership Idaho Agriculture", sugar_filtered$sponsors_cleaned, ignore.case = TRUE), ]
florida_sugar <- sugar_filtered[grepl("Florida", sugar_filtered$sponsors_cleaned, ignore.case = TRUE), ]
ag_inst_florida_sugar <- sugar_filtered[grepl("Agricultural Institute Of Florida Foundation", sugar_filtered$sponsors_cleaned, ignore.case = TRUE), ]
midwest_sugar |>
group_by(Year) |>
summarize(trip_count = n()) |>
arrange(desc(trip_count))
sugar_filtered |>
group_by(MemberName) |>
summarize(number_trips = n()) |>
arrange(desc(number_trips))
sugar_filtered |>
filter(MemberName == "Bacon, Don")
biofuels_filtered |>
group_by(MemberName) |>
summarize(number_trips = n()) |>
arrange(desc(number_trips))
sugar_filtered |>
filter(MemberName == "Yoho, Ted S.") |>
group_by(sponsors_cleaned) |>
summarize(number_trips = n()) |>
arrange(desc(number_trips))
setwd("C:/Users/paulf/GitHub/congressional-private-travel/house_travel/sugar")
setwd("C:/Users/paulf/GitHub/congressional-private-travel/house_travel")
write_csv(sugar_travelers, "sugar/sugar_travelers.csv")
sugar_travelers <- sugar_filtered |>
group_by(FilerName) |>
summarize(number_trips = n()) |>
arrange(desc(number_trips))
print(sugar_travelers)
View(sugar_filtered)
setwd("C:/Users/paulf/GitHub/INST-737")
knitr::opts_chunk$set(echo = TRUE)
# Read in the initial dataset, setting a guess_max limit to improve the accuracy of the data types assigned to each column.
pg_foreclosures_per_tract <- read_csv("datasets/pg_foreclosures_per_tract.csv", guess_max = 173)
# Create a new column "foreclosure_quantile" based on quantiles of foreclosure_pc_2020
pg_foreclosures_per_tract_log_reg <- pg_foreclosures_per_tract %>%
mutate(foreclosure_quantile = ntile(foreclosure_pc_2020, 5))
# Set the seed for reproducibility
set.seed(123)
# Split the data into 70% training and 30% test sets
train_index <- createDataPartition(pg_foreclosures_per_tract_log_reg$foreclosure_quantile, p = 0.7, list = FALSE)
# Create training and test sets
train_data <- pg_foreclosures_per_tract_log_reg[train_index, ]
test_data <- pg_foreclosures_per_tract_log_reg[-train_index, ]
# Check the dimensions of training and test sets
dim(train_data)
dim(test_data)
# Create scatterplot matrix
pairs(train_data[, c("medincome_change_2010_2020", "tract_medincome_2020", "tract_medage_2020", "nhwhite_change_2010_2020", "mortgage_change_2010_2020", "mortgaged_2020", "poverty_2020", "pct_built_pre_1960", "great_recession_foreclosures")])
# Check data range for numeric variables
numeric_vars <- train_data[sapply(train_data, is.numeric)]
data_range <- sapply(numeric_vars, function(x) c(min = min(x, na.rm = TRUE), max = max(x, na.rm = TRUE)))
print("Data Range:")
print(data_range)
# Check for infinite values in numeric variables
infinite_values <- colSums(sapply(numeric_vars, is.infinite))
print("Infinite Values:")
print(infinite_values)
# Check for collinear variables (using correlation matrix)
correlation_matrix <- cor(numeric_vars)
highly_correlated <- findCorrelation(correlation_matrix, cutoff = 0.8)
print("Highly Correlated Variables:")
print(colnames(numeric_vars)[highly_correlated])
# Convert foreclosure_quantile to factor
train_data$foreclosure_quantile <- factor(train_data$foreclosure_quantile)
# Use the training dataset for model fitting
model <- polr(foreclosure_quantile ~ medincome_change_2010_2020 + tract_medage_2020 + nhwhite_change_2010_2020 + medincome_change_2010_2020 + great_recession_foreclosures, data = train_data, Hess = TRUE)
# Summarize the model
summary(model)
# Use the training dataset for model fitting
model <- polr(foreclosure_quantile ~ avg_bed + pct_built_2010_2019 + pct_built_pre_1960 + mortgage_change_2010_2020, data = train_data, Hess = TRUE)
# Summarize the model
summary(model)
# Use the training dataset for model fitting
model <- polr(foreclosure_quantile ~ medincome_change_2010_2020 + tract_medage_2020 + nhwhite_change_2010_2020 + great_recession_foreclosures + avg_bed + pct_built_2010_2019 + pct_built_pre_1960 + mortgage_change_2010_2020, data = train_data, Hess = TRUE)
# Summarize the model
summary(model)
# Convert foreclosure_quantile to factor in the original dataset
train_data$foreclosure_quantile <- factor(train_data$foreclosure_quantile)
# Fit the model using the original dataset
model <- polr(foreclosure_quantile ~ tract_medage_2020 + nhwhite_2020 + mortgaged_2020 +  great_recession_foreclosures, data = train_data, Hess = TRUE)
# Summarize the model
summary(model)
set.seed(123) # For reproducibility
test_percent <- 0.2
indices <- sample(1:nrow(pg_foreclosures_per_tract_log_reg),
size = round(test_percent * nrow(pg_foreclosures_per_tract_log_reg)))
train_data <- pg_foreclosures_per_tract_log_reg[-indices, ]
test_data <- pg_foreclosures_per_tract_log_reg[indices, ]
# Step 2: Train Naive Bayes Model
nb_model <- naiveBayes(foreclosure_quantile ~ tract_medage_2020 + nhwhite_2020 + mortgaged_2020 + great_recession_foreclosures,
data = train_data)
# Step 3: Make Predictions on Test Data
predictions <- predict(nb_model, newdata = test_data)
# Step 4: Evaluate Model Performance
confusionMatrix(predictions, test_data$foreclosure_quantile)
# Selecting the variables of interest
variables_of_interest <- c("avg_bed", "tract_homevalue_2020", "tract_medage_2020", "tract_medincome_2020",
"tract_medincome_2010", "foreclosure_pc_2020", "foreclosure_pc_2010",
"pct_built_2020_later", "pct_built_2010_2019", "pct_built_2000_2009",
"pct_built_1990_1999", "pct_built_1980_1989", "pct_built_1970_1979",
"pct_built_pre_1960", "pct_0_bd", "pct_1_bd", "pct_2_bd", "pct_3_bd",
"pct_4_more_bd", "poverty_2010", "poverty_2020", "nhwhite_2010", "nhwhite_2020",
"mortgaged_2010", "mortgaged_2015", "mortgaged_2020", "ownoccupied_2010",
"ownoccupied_2015", "ownoccupied_2020", "mortgage_change_2010_2015",
"mortgage_change_2015_2020", "mortgage_change_2010_2020", "ownoccupied_change_2010_2015",
"ownoccupied_change_2015_2020", "ownoccupied_change_2010_2020", "poverty_change_2010_2020",
"nhwhite_change_2010_2020", "medincome_change_2010_2015", "medincome_change_2015_2020",
"medincome_change_2010_2020")
# Calculating the correlation matrix
correlation_matrix <- cor(pg_foreclosures_per_tract[variables_of_interest])
# Print the correlation matrix
print(correlation_matrix)
# Selecting the variables of interest (excluding foreclosure_pc_2020)
variables_of_interest <- c("avg_bed", "tract_homevalue_2020", "tract_medage_2020", "tract_medincome_2020",
"tract_medincome_2010", "foreclosure_pc_2010",
"pct_built_2020_later", "pct_built_2010_2019", "pct_built_2000_2009",
"pct_built_1990_1999", "pct_built_1980_1989", "pct_built_1970_1979",
"pct_built_pre_1960", "pct_0_bd", "pct_1_bd", "pct_2_bd", "pct_3_bd",
"pct_4_more_bd", "poverty_2010", "poverty_2020", "nhwhite_2010", "nhwhite_2020",
"mortgaged_2010", "mortgaged_2015", "mortgaged_2020", "ownoccupied_2010",
"ownoccupied_2015", "ownoccupied_2020", "mortgage_change_2010_2015",
"mortgage_change_2015_2020", "mortgage_change_2010_2020", "ownoccupied_change_2010_2015",
"ownoccupied_change_2015_2020", "ownoccupied_change_2010_2020", "poverty_change_2010_2020",
"nhwhite_change_2010_2020", "medincome_change_2010_2015", "medincome_change_2015_2020",
"medincome_change_2010_2020")
# Create scatterplot matrix
pairs(~ foreclosure_pc_2020 + pg_foreclosures_per_tract[variables_of_interest], data = pg_foreclosures_per_tract)
# Selecting the variables of interest (excluding foreclosure_pc_2020)
variables_of_interest <- c("avg_bed", "tract_homevalue_2020", "tract_medage_2020", "tract_medincome_2020",
"tract_medincome_2010", "foreclosure_pc_2010",
"pct_built_2020_later", "pct_built_2010_2019", "pct_built_2000_2009",
"pct_built_1990_1999", "pct_built_1980_1989", "pct_built_1970_1979",
"pct_built_pre_1960", "pct_0_bd", "pct_1_bd", "pct_2_bd", "pct_3_bd",
"pct_4_more_bd", "poverty_2010", "poverty_2020", "nhwhite_2010", "nhwhite_2020",
"mortgaged_2010", "mortgaged_2015", "mortgaged_2020", "ownoccupied_2010",
"ownoccupied_2015", "ownoccupied_2020", "mortgage_change_2010_2015",
"mortgage_change_2015_2020", "mortgage_change_2010_2020", "ownoccupied_change_2010_2015",
"ownoccupied_change_2015_2020", "ownoccupied_change_2010_2020", "poverty_change_2010_2020",
"nhwhite_change_2010_2020", "medincome_change_2010_2015", "medincome_change_2015_2020",
"medincome_change_2010_2020")
# Create a new dataframe with variables of interest and foreclosure_pc_2020
data_subset <- pg_foreclosures_per_tract[c("foreclosure_pc_2020", variables_of_interest)]
# Create scatterplot matrix
pairs(data_subset)
# Select a subset of variables of interest (excluding foreclosure_pc_2020)
variables_of_interest_subset <- c("avg_bed", "tract_homevalue_2020", "tract_medage_2020", "tract_medincome_2020",
"tract_medincome_2010", "foreclosure_pc_2010")
# Create a new dataframe with variables of interest and foreclosure_pc_2020
data_subset <- pg_foreclosures_per_tract[c("foreclosure_pc_2020", variables_of_interest_subset)]
# Create scatterplot matrix
pairs(data_subset)
# Selecting the variables of interest (excluding foreclosure_pc_2020)
variables_of_interest <- c("avg_bed", "tract_homevalue_2020", "tract_medage_2020", "tract_medincome_2020",
"tract_medincome_2010", "foreclosure_pc_2010",
"pct_built_2020_later", "pct_built_2010_2019", "pct_built_2000_2009",
"pct_built_1990_1999", "pct_built_1980_1989", "pct_built_1970_1979",
"pct_built_pre_1960", "pct_0_bd", "pct_1_bd", "pct_2_bd", "pct_3_bd",
"pct_4_more_bd", "poverty_2010", "poverty_2020", "nhwhite_2010", "nhwhite_2020",
"mortgaged_2010", "mortgaged_2015", "mortgaged_2020", "ownoccupied_2010",
"ownoccupied_2015", "ownoccupied_2020", "mortgage_change_2010_2015",
"mortgage_change_2015_2020", "mortgage_change_2010_2020", "ownoccupied_change_2010_2015",
"ownoccupied_change_2015_2020", "ownoccupied_change_2010_2020", "poverty_change_2010_2020",
"nhwhite_change_2010_2020", "medincome_change_2010_2015", "medincome_change_2015_2020",
"medincome_change_2010_2020")
# Calculate the correlation matrix
correlation_matrix <- cor(pg_foreclosures_per_tract[variables_of_interest], pg_foreclosures_per_tract$foreclosure_pc_2020)
# Print the correlation coefficients
print(correlation_matrix)
library(gplots)
install.packages('gplots')
library(gplots)
# Calculate the correlation matrix
correlation_matrix <- cor(pg_foreclosures_per_tract[variables_of_interest], pg_foreclosures_per_tract$foreclosure_pc_2020)
# Create a heatmap
heatmap.2(correlation_matrix,
trace = "none",        # no borders around heatmap
col = colorRampPalette(c("blue", "white", "red"))(100),  # color palette
margins = c(12, 9),    # more space for row and column labels
main = "Correlation Heatmap")  # title
# Calculate the correlation matrix
correlation_matrix <- cor(pg_foreclosures_per_tract[variables_of_interest], pg_foreclosures_per_tract$foreclosure_pc_2020)
# Create a heatmap
heatmap.2(correlation_matrix,
trace = "none",        # no borders around heatmap
col = colorRampPalette(c("blue", "white", "red"))(100),  # color palette
margins = c(12, 9),    # more space for row and column labels
main = "Correlation Heatmap")  # title
# Calculate the correlation matrix
correlation_matrix <- cor(pg_foreclosures_per_tract[variables_of_interest], pg_foreclosures_per_tract$foreclosure_pc_2020)
# Transpose the correlation matrix
correlation_matrix <- t(correlation_matrix)
# Create a heatmap
heatmap.2(correlation_matrix,
trace = "none",        # no borders around heatmap
col = colorRampPalette(c("blue", "white", "red"))(100),  # color palette
margins = c(12, 9),    # more space for row and column labels
main = "Correlation Heatmap")  # title
# Calculate the correlation coefficients
correlation_coefficients <- cor(pg_foreclosures_per_tract[variables_of_interest], pg_foreclosures_per_tract$foreclosure_pc_2020)
# Convert the correlation coefficients to a data frame
correlation_df <- as.data.frame(correlation_coefficients)
# Exclude the diagonal (correlation of a variable with itself)
correlation_df <- correlation_df[upper.tri(correlation_df)]
# Plot the correlations
plot(correlation_df,
main = "Correlation of Variables with foreclosure_pc_2020",
xlab = "Variable",
ylab = "Correlation Coefficient")
# Calculate the correlation coefficients
correlation_coefficients <- cor(pg_foreclosures_per_tract[variables_of_interest], pg_foreclosures_per_tract$foreclosure_pc_2020)
# Convert the correlation coefficients to a data frame
correlation_df <- as.data.frame(correlation_coefficients)
# Exclude the diagonal (correlation of a variable with itself)
correlation_df <- correlation_df[upper.tri(correlation_df)]
# Plot the correlations
plot(correlation_df,
ylim = c(-1, 1), # Set y-axis limits
main = "Correlation of Variables with foreclosure_pc_2020",
xlab = "Variable",
ylab = "Correlation Coefficient")
knitr::opts_chunk$set(echo = TRUE)
options(scipen=999)
##install.packages("ggrepel")
##install.packages('ggthemes')
##install.packages('gplots')
library(tidyverse)
library(dplyr)
#Necessary for date parsing.
library(lubridate)
#Necessary for column name cleaning.
library(janitor)
#Eventually necessary for visualization
library(ggthemes)
#Necessary for Census API calls.
library(tidycensus)
#Eventually necessary for visualizations.
library(ggplot2)
library(gplots)
#Potentially necessary for cleaning.
library(ggrepel)
#Eventually necessary for visualizations.
library(tigris)
#Simple features.
library(sf)
#Eventually necessary for mapping.
library(ggmap)
# Read in the initial dataset, setting a guess_max limit to improve the accuracy of the data types assigned to each column.
foreclosure_pg <- read_csv("datasets/pg_foreclosures.csv", guess_max = 71676)
#Read in the geocoded address dataset.
foreclosure_pg_census_tracts <- read_csv("datasets/pg_foreclosure_tract_geocodio.csv", guess_max = 71676) |>
clean_names()
# Calculate the number of unique tracts in the geocoded address dataframe.
tracts_unique <- length(unique(foreclosure_pg_census_tracts$census_tract_code))
print(tracts_unique)
# Perform inner join based on the 'location' column.
pg_foreclosures_with_tracts <- inner_join(foreclosure_pg, foreclosure_pg_census_tracts, by = "location")
# Convert "submitteddate" column to date type.
pg_foreclosures_with_tracts$submitteddate <- as.Date(pg_foreclosures_with_tracts$submitteddate, format = "%m/%d/%Y")
# Extract the year and month from "submitteddate".
pg_foreclosures_with_tracts$year <- year(pg_foreclosures_with_tracts$submitteddate)
pg_foreclosures_with_tracts$month <- month(pg_foreclosures_with_tracts$submitteddate)
# Create a year-month column for eventual plotting.
pg_foreclosures_with_tracts$year_month <- as.Date(paste(pg_foreclosures_with_tracts$year, pg_foreclosures_with_tracts$month, "01", sep = "-"), format = "%Y-%m-%d")
# Print the dataset.
print(pg_foreclosures_with_tracts)
# Order the dataframe by the columns that identify remainins duplicate rows.
pg_foreclosures_with_tracts <- pg_foreclosures_with_tracts[order(pg_foreclosures_with_tracts$propertyid, pg_foreclosures_with_tracts$street_address, pg_foreclosures_with_tracts$city.y, pg_foreclosures_with_tracts$submitteddate), ]
# Keep only the last row from each set of duplicates
pg_foreclosures_filtered <- subset(pg_foreclosures_with_tracts, !duplicated(pg_foreclosures_with_tracts[, c("propertyid", "street_address", "city.y", "submitteddate")], fromLast = TRUE))
# Reset row names if necessary
rownames(pg_foreclosures_filtered) <- NULL
# Filter the pg_foreclosures_filtered dataset to remove rows with missing values in the census_tract_code column.
pg_foreclosures_filtered <- pg_foreclosures_filtered[!is.na(pg_foreclosures_filtered$census_tract_code), ]
# Remove unnecessary columns with the select() function.
pg_foreclosures_filtered <- pg_foreclosures_filtered %>%
select(-accuracy_score, -accuracy_type, -source, -full_fips_block, -metro_micro_statistical_area_name,-metro_micro_statistical_area_code, -metro_micro_statistical_area_type, -combined_statistical_area_name, -metropolitan_division_area_name, -metropolitan_division_area_code, -addressoccupied)
knitr::opts_chunk$set(echo = TRUE)
options(scipen=999)
##install.packages("ggrepel")
##install.packages('ggthemes')
##install.packages('gplots')
library(tidyverse)
library(dplyr)
#Necessary for date parsing.
library(lubridate)
#Necessary for column name cleaning.
library(janitor)
#Eventually necessary for visualization
library(ggthemes)
#Necessary for Census API calls.
library(tidycensus)
#Eventually necessary for visualizations.
library(ggplot2)
library(gplots)
#Potentially necessary for cleaning.
library(ggrepel)
#Eventually necessary for visualizations.
library(tigris)
#Simple features.
library(sf)
#Eventually necessary for mapping.
library(ggmap)
# Read in the initial dataset, setting a guess_max limit to improve the accuracy of the data types assigned to each column.
foreclosure_pg <- read_csv("datasets/pg_foreclosures.csv", guess_max = 71676)
#Read in the geocoded address dataset.
foreclosure_pg_census_tracts <- read_csv("datasets/pg_foreclosure_tract_geocodio.csv", guess_max = 71676) |>
clean_names()
# Calculate the number of unique tracts in the geocoded address dataframe.
tracts_unique <- length(unique(foreclosure_pg_census_tracts$census_tract_code))
print(tracts_unique)
# Perform inner join based on the 'location' column.
pg_foreclosures_with_tracts <- inner_join(foreclosure_pg, foreclosure_pg_census_tracts, by = "location")
# Convert "submitteddate" column to date type.
pg_foreclosures_with_tracts$submitteddate <- as.Date(pg_foreclosures_with_tracts$submitteddate, format = "%m/%d/%Y")
# Extract the year and month from "submitteddate".
pg_foreclosures_with_tracts$year <- year(pg_foreclosures_with_tracts$submitteddate)
pg_foreclosures_with_tracts$month <- month(pg_foreclosures_with_tracts$submitteddate)
# Create a year-month column for eventual plotting.
pg_foreclosures_with_tracts$year_month <- as.Date(paste(pg_foreclosures_with_tracts$year, pg_foreclosures_with_tracts$month, "01", sep = "-"), format = "%Y-%m-%d")
# Print the dataset.
print(pg_foreclosures_with_tracts)
# Order the dataframe by the columns that identify remainins duplicate rows.
pg_foreclosures_with_tracts <- pg_foreclosures_with_tracts[order(pg_foreclosures_with_tracts$propertyid, pg_foreclosures_with_tracts$street_address, pg_foreclosures_with_tracts$city.y, pg_foreclosures_with_tracts$submitteddate), ]
# Keep only the last row from each set of duplicates
pg_foreclosures_filtered <- subset(pg_foreclosures_with_tracts, !duplicated(pg_foreclosures_with_tracts[, c("propertyid", "street_address", "city.y", "submitteddate")], fromLast = TRUE))
# Reset row names if necessary
rownames(pg_foreclosures_filtered) <- NULL
# Filter the pg_foreclosures_filtered dataset to remove rows with missing values in the census_tract_code column.
pg_foreclosures_filtered <- pg_foreclosures_filtered[!is.na(pg_foreclosures_filtered$census_tract_code), ]
# Remove unnecessary columns with the select() function.
pg_foreclosures_filtered <- pg_foreclosures_filtered %>%
select(-accuracy_score, -accuracy_type, -source, -full_fips_block, -metro_micro_statistical_area_name,-metro_micro_statistical_area_code, -metro_micro_statistical_area_type, -combined_statistical_area_name, -metropolitan_division_area_name, -metropolitan_division_area_code, -addressoccupied)
