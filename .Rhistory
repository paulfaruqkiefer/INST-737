library(sf)
#Eventually necessary for mapping.
library(ggmap)
# Assuming pg_foreclosures_per_tract is your dataframe
missing_values_count <- colSums(is.na(pg_foreclosures_per_tract))
# Print the number of missing values in each column
print(missing_values_count)
knitr::opts_chunk$set(echo = TRUE)
# Create partial dependence data
pdp_age <- partial(model, pred.var = "tract_medage_2020")
knitr::opts_chunk$set(echo = TRUE)
# Function to calculate partial dependence manually
partial_dependence <- function(model, data, pred_var, values) {
# Make a copy of the data
data_copy <- data
# Iterate over the specified values
for (val in values) {
# Update the specified predictor variable with the current value
data_copy[, pred_var] <- val
# Predict using the updated data and accumulate the predictions
if (!exists("predictions")) {
predictions <- predict(model, newdata = data_copy, type = "probs")
} else {
predictions <- predictions + predict(model, newdata = data_copy, type = "probs")
}
}
# Average the predictions
predictions <- predictions / length(values)
# Return the predictions
return(predictions)
}
# Specify values for tract_medage_2020
age_values <- seq(min(train_data$tract_medage_2020), max(train_data$tract_medage_2020), length.out = 100)
# Calculate partial dependence manually
partial_predictions <- partial_dependence(model, data = train_data, pred_var = "tract_medage_2020", values = age_values)
# Create data frame for plotting
partial_df <- data.frame(tract_medage_2020 = age_values, partial_predictions)
knitr::opts_chunk$set(echo = TRUE)
# Check the length of age_values and partial_predictions
length_age_values <- length(age_values)
length_partial_predictions <- nrow(partial_predictions)
# Ensure lengths are consistent
if (length_age_values != length_partial_predictions) {
# Adjust the length of age_values to match partial_predictions
age_values <- seq(min(train_data$tract_medage_2020), max(train_data$tract_medage_2020), length.out = length_partial_predictions)
}
# Create data frame for plotting
partial_df <- data.frame(tract_medage_2020 = age_values, partial_predictions)
# Plot partial dependence using ggplot2
ggplot(partial_df, aes(x = tract_medage_2020, y = partial_predictions)) +
geom_line() +
labs(title = "Partial Dependence Plot for Tract Median Age",
x = "Tract Median Age 2020",
y = "Predicted Probability")
knitr::opts_chunk$set(echo = TRUE)
# Check the length of age_values
length_age_values <- length(age_values)
# Ensure the length of partial_predictions matches age_values
if (length_age_values != nrow(partial_predictions)) {
stop("Length mismatch between age_values and partial_predictions")
}
# Create data frame for plotting
partial_df <- data.frame(tract_medage_2020 = age_values, partial_predictions)
# Plot partial dependence using ggplot2
ggplot(partial_df, aes(x = tract_medage_2020, y = partial_predictions)) +
geom_line() +
labs(title = "Partial Dependence Plot for Tract Median Age",
x = "Tract Median Age 2020",
y = "Predicted Probability")
knitr::opts_chunk$set(echo = TRUE)
# Check the structure and dimensions of partial_predictions
str(partial_predictions)
knitr::opts_chunk$set(echo = TRUE)
# Check the class of age_values
class(age_values)
# Check the class of partial_predictions
class(partial_predictions)
knitr::opts_chunk$set(echo = TRUE)
# Extracting the column from partial_predictions
partial_predictions <- partial_predictions[, 1]
# Create data frame for plotting
partial_df <- data.frame(tract_medage_2020 = age_values, partial_predictions)
# Plot partial dependence using ggplot2
ggplot(partial_df, aes(x = tract_medage_2020, y = partial_predictions)) +
geom_line() +
labs(title = "Partial Dependence Plot for Tract Median Age",
x = "Tract Median Age 2020",
y = "Predicted Probability")
# Replace missing values in tract_homevalue_2020 with the mean of the column
mean_value <- mean(pg_foreclosures_per_tract$tract_homevalue_2020, na.rm = TRUE)
pg_foreclosures_per_tract$tract_homevalue_2020[is.na(pg_foreclosures_per_tract$tract_homevalue_2020)] <- mean_value
write_csv(pg_foreclosures_per_tract, "datasets/pg_foreclosures_per_tract.csv")
knitr::opts_chunk$set(echo = TRUE)
# Read in the initial dataset, setting a guess_max limit to improve the accuracy of the data types assigned to each column.
pg_foreclosures_per_tract <- read_csv("datasets/pg_foreclosures_per_tract.csv", guess_max = 173)
# Create a new column "foreclosure_quantile" based on quantiles of foreclosure_pc_2020
pg_foreclosures_per_tract_log_reg <- pg_foreclosures_per_tract %>%
mutate(foreclosure_quantile = ntile(foreclosure_pc_2020, 5))
# Set the seed for reproducibility
set.seed(123)
# Split the data into 70% training and 30% test sets
train_index <- createDataPartition(pg_foreclosures_per_tract_log_reg$foreclosure_quantile, p = 0.7, list = FALSE)
knitr::opts_chunk$set(echo = TRUE)
# Read in the initial dataset, setting a guess_max limit to improve the accuracy of the data types assigned to each column.
pg_foreclosures_per_tract <- read_csv("datasets/pg_foreclosures_per_tract.csv", guess_max = 173)
# Create a new column "foreclosure_quantile" based on quantiles of foreclosure_pc_2020
pg_foreclosures_per_tract_log_reg <- pg_foreclosures_per_tract %>%
mutate(foreclosure_quantile = ntile(foreclosure_pc_2020, 5))
# Set the seed for reproducibility
set.seed(123)
# Split the data into 70% training and 30% test sets
train_index <- createDataPartition(pg_foreclosures_per_tract_log_reg$foreclosure_quantile, p = 0.7, list = FALSE)
knitr::opts_chunk$set(echo = TRUE)
options(scipen=999)
##install.packages("ggrepel")
##install.packages('ggthemes')
##install.packages('aod')
##install.packages('caret')
##install.packages("MASS")
##install.packages("mice")
install.packages("pdp")
library(tidyverse)
#Necessary for date parsing.
library(lubridate)
#Necessary for column name cleaning.
library(janitor)
#Eventually necessary for visualization
library(ggthemes)
#Necessary for Census API calls.
library(tidycensus)
#Eventually necessary for visualizations.
library(ggplot2)
#Potentially necessary for cleaning.
library(ggrepel)
#Eventually necessary for visualizations.
library(tigris)
#Simple features.
library(sf)
#Eventually necessary for mapping.
library(ggmap)
#Needed for logistic regression
library(aod)
#Needed for splitting into training and test datasets
library(caret)
#Needed for ordinal logistic regression
library(MASS)
#Needed for imputing missing values (using predictive mean matching)
library(mice)
#Needed for visualizing a partial dependence plot
library(pdp)
install.packages("pdp")
knitr::opts_chunk$set(echo = TRUE)
# Read in the initial dataset, setting a guess_max limit to improve the accuracy of the data types assigned to each column.
pg_foreclosures_per_tract <- read_csv("datasets/pg_foreclosures_per_tract.csv", guess_max = 173)
knitr::opts_chunk$set(echo = TRUE)
options(scipen=999)
##install.packages("ggrepel")
##install.packages('ggthemes')
##install.packages('aod')
##install.packages('caret')
##install.packages("MASS")
##install.packages("mice")
install.packages("pdp")
library(tidyverse)
#Necessary for date parsing.
library(lubridate)
#Necessary for column name cleaning.
library(janitor)
#Eventually necessary for visualization
library(ggthemes)
#Necessary for Census API calls.
library(tidycensus)
#Eventually necessary for visualizations.
library(ggplot2)
#Potentially necessary for cleaning.
library(ggrepel)
#Eventually necessary for visualizations.
library(tigris)
#Simple features.
library(sf)
#Eventually necessary for mapping.
library(ggmap)
#Needed for logistic regression
library(aod)
#Needed for splitting into training and test datasets
library(caret)
#Needed for ordinal logistic regression
library(MASS)
#Needed for imputing missing values (using predictive mean matching)
library(mice)
#Needed for visualizing a partial dependence plot
library(pdp)
knitr::opts_chunk$set(echo = TRUE)
# Read in the initial dataset, setting a guess_max limit to improve the accuracy of the data types assigned to each column.
pg_foreclosures_per_tract <- read_csv("datasets/pg_foreclosures_per_tract.csv", guess_max = 173)
# Create a new column "foreclosure_quantile" based on quantiles of foreclosure_pc_2020
pg_foreclosures_per_tract_log_reg <- pg_foreclosures_per_tract %>%
mutate(foreclosure_quantile = ntile(foreclosure_pc_2020, 5))
# Set the seed for reproducibility
set.seed(123)
# Split the data into 70% training and 30% test sets
train_index <- createDataPartition(pg_foreclosures_per_tract_log_reg$foreclosure_quantile, p = 0.7, list = FALSE)
# Create training and test sets
train_data <- pg_foreclosures_per_tract_log_reg[train_index, ]
test_data <- pg_foreclosures_per_tract_log_reg[-train_index, ]
# Check the dimensions of training and test sets
dim(train_data)
dim(test_data)
# Create scatterplot matrix
pairs(train_data[, c("medincome_change_2010_2020", "tract_medincome_2020", "tract_medage_2020", "nhwhite_change_2010_2020", "mortgage_change_2010_2020", "mortgaged_2020", "poverty_2020", "pct_built_pre_1960", "great_recession_foreclosures")])
# Convert foreclosure_quantile to factor
train_data$foreclosure_quantile <- factor(train_data$foreclosure_quantile)
# Use the imputed dataset for model fitting
model <- polr(foreclosure_quantile ~ medincome_change_2010_2020 + tract_medage_2020 + nhwhite_change_2010_2020 + pct_built_pre_1960 + medincome_change_2010_2020 + great_recession_foreclosures, data = train_data, Hess = TRUE)
# Summarize the model
summary(model)
# Check the structure and dimensions of partial_predictions
str(partial_predictions)
knitr::opts_chunk$set(echo = TRUE)
# Function to calculate partial dependence manually
partial_dependence <- function(model, data, pred_var, values) {
# Make a copy of the data
data_copy <- data
# Iterate over the specified values
for (val in values) {
# Update the specified predictor variable with the current value
data_copy[, pred_var] <- val
# Predict using the updated data and accumulate the predictions
if (!exists("predictions")) {
predictions <- predict(model, newdata = data_copy, type = "probs")
} else {
predictions <- predictions + predict(model, newdata = data_copy, type = "probs")
}
}
# Average the predictions
predictions <- predictions / length(values)
# Return the predictions
return(predictions)
}
# Specify values for tract_medage_2020
age_values <- seq(min(train_data$tract_medage_2020), max(train_data$tract_medage_2020), length.out = 100)
# Calculate partial dependence manually
partial_predictions <- partial_dependence(model, data = train_data, pred_var = "tract_medage_2020", values = age_values)
# Create data frame for plotting
partial_df <- data.frame(tract_medage_2020 = age_values, partial_predictions)
knitr::opts_chunk$set(echo = TRUE)
# Check the structure and dimensions of partial_predictions
str(partial_predictions)
# Check the class of age_values
class(age_values)
# Check the class of partial_predictions
class(partial_predictions)
# Extracting the column from partial_predictions
partial_predictions <- partial_predictions[, 1]
# Create data frame for plotting
partial_df <- data.frame(tract_medage_2020 = age_values, partial_predictions)
knitr::opts_chunk$set(echo = TRUE)
# Extracting the column from partial_predictions
partial_predictions <- partial_predictions[, 1]
knitr::opts_chunk$set(echo = TRUE)
# Calculate Partial Dependence for tract_medage_2020
# Assuming partial_predictions is a matrix array containing predicted probabilities
# For each category of the response variable
# Replace `age_values` with your sequence of values for tract_medage_2020
age_values <- seq(min(train_data$tract_medage_2020), max(train_data$tract_medage_2020), length.out = 100)
partial_predictions <- partial(model, pred.var = "tract_medage_2020", grid.levels = list(tract_medage_2020 = age_values))
# 3. Prepare Data for Visualization
# Extracting the column from partial_predictions
partial_predictions <- partial_predictions[, 1]
# Create a data frame for plotting
plot_data <- data.frame(tract_medage_2020 = age_values, predicted_probability = partial_predictions)
knitr::opts_chunk$set(echo = TRUE)
# Calculate partial dependence with correct number of values
partial_predictions <- partial(model, pred.var = "tract_medage_2020", grid.levels = list(tract_medage_2020 = age_values), train = train_data)
# Extracting the column from partial_predictions
partial_predictions <- partial_predictions[, 1]
# Check if the number of rows match
if (length(age_values) != nrow(partial_predictions)) {
# Adjust the number of values for age_values
age_values <- seq(min(train_data$tract_medage_2020), max(train_data$tract_medage_2020), length.out = nrow(partial_predictions))
}
knitr::opts_chunk$set(echo = TRUE)
# Calculate partial dependence with correct number of values
partial_predictions <- partial(model, pred.var = "tract_medage_2020", grid.levels = list(tract_medage_2020 = age_values), train = train_data)
# Check if there are any warnings
if (any(warnings())) {
print("Warnings occurred during partial dependence calculation:")
print(warnings())
}
# Extracting the column from partial_predictions
partial_predictions <- partial_predictions[, 1]
knitr::opts_chunk$set(echo = TRUE)
# Calculate partial dependence with correct number of values
partial_predictions <- partial(model, pred.var = "tract_medage_2020",
grid.levels = list(tract_medage_2020 = age_values), train = train_data)
# Check for warnings
if (any(warnings())) {
print("Warnings occurred during partial dependence calculation:")
print(warnings())
}
# Extracting the column from partial_predictions
partial_predictions <- partial_predictions[, 1]
# Check the number of rows in partial_predictions
if (length(age_values) != nrow(partial_predictions)) {
stop("Length mismatch between age_values and partial_predictions")
}
knitr::opts_chunk$set(echo = TRUE)
options(scipen=999)
##install.packages("ggrepel")
##install.packages('ggthemes')
library(tidyverse)
#Necessary for date parsing.
library(lubridate)
#Necessary for column name cleaning.
library(janitor)
#Eventually necessary for visualization
library(ggthemes)
#Necessary for Census API calls.
library(tidycensus)
#Eventually necessary for visualizations.
library(ggplot2)
#Potentially necessary for cleaning.
library(ggrepel)
#Eventually necessary for visualizations.
library(tigris)
#Simple features.
library(sf)
#Eventually necessary for mapping.
library(ggmap)
write_csv(pg_foreclosures_per_tract, "datasets/pg_foreclosures_per_tract.csv")
knitr::opts_chunk$set(echo = TRUE)
options(scipen=999)
##install.packages("ggrepel")
##install.packages('ggthemes')
library(tidyverse)
#Necessary for date parsing.
library(lubridate)
#Necessary for column name cleaning.
library(janitor)
#Eventually necessary for visualization
library(ggthemes)
#Necessary for Census API calls.
library(tidycensus)
#Eventually necessary for visualizations.
library(ggplot2)
#Potentially necessary for cleaning.
library(ggrepel)
#Eventually necessary for visualizations.
library(tigris)
#Simple features.
library(sf)
#Eventually necessary for mapping.
library(ggmap)
# Read in the initial dataset, setting a guess_max limit to improve the accuracy of the data types assigned to each column.
foreclosure_pg <- read_csv("datasets/pg_foreclosures.csv", guess_max = 71676)
#Read in the geocoded address dataset.
foreclosure_pg_census_tracts <- read_csv("datasets/pg_foreclosure_tract_geocodio.csv", guess_max = 71676) |>
clean_names()
# Calculate the number of unique tracts in the geocoded address dataframe.
tracts_unique <- length(unique(foreclosure_pg_census_tracts$census_tract_code))
print(tracts_unique)
# Perform inner join based on the 'location' column.
pg_foreclosures_with_tracts <- inner_join(foreclosure_pg, foreclosure_pg_census_tracts, by = "location")
# Convert "submitteddate" column to date type.
pg_foreclosures_with_tracts$submitteddate <- as.Date(pg_foreclosures_with_tracts$submitteddate, format = "%m/%d/%Y")
# Extract the year and month from "submitteddate".
pg_foreclosures_with_tracts$year <- year(pg_foreclosures_with_tracts$submitteddate)
pg_foreclosures_with_tracts$month <- month(pg_foreclosures_with_tracts$submitteddate)
# Create a year-month column for eventual plotting.
pg_foreclosures_with_tracts$year_month <- as.Date(paste(pg_foreclosures_with_tracts$year, pg_foreclosures_with_tracts$month, "01", sep = "-"), format = "%Y-%m-%d")
# Print the dataset.
print(pg_foreclosures_with_tracts)
# Order the dataframe by the columns that identify remainins duplicate rows.
pg_foreclosures_with_tracts <- pg_foreclosures_with_tracts[order(pg_foreclosures_with_tracts$propertyid, pg_foreclosures_with_tracts$street_address, pg_foreclosures_with_tracts$city.y, pg_foreclosures_with_tracts$submitteddate), ]
# Keep only the last row from each set of duplicates
pg_foreclosures_filtered <- subset(pg_foreclosures_with_tracts, !duplicated(pg_foreclosures_with_tracts[, c("propertyid", "street_address", "city.y", "submitteddate")], fromLast = TRUE))
# Reset row names if necessary
rownames(pg_foreclosures_filtered) <- NULL
# Filter the pg_foreclosures_filtered dataset to remove rows with missing values in the census_tract_code column.
pg_foreclosures_filtered <- pg_foreclosures_filtered[!is.na(pg_foreclosures_filtered$census_tract_code), ]
# Remove unnecessary columns with the select() function.
pg_foreclosures_filtered <- select(pg_foreclosures_filtered, -accuracy_score, -accuracy_type, -source, -full_fips_block, -metro_micro_statistical_area_name,-metro_micro_statistical_area_code, -metro_micro_statistical_area_type, -combined_statistical_area_name, -metropolitan_division_area_name, -metropolitan_division_area_code, -addressoccupied)
# Remove unnecessary columns with the select() function.
pg_foreclosures_filtered <- pg_foreclosures_filtered |>
select(pg_foreclosures_filtered, -accuracy_score, -accuracy_type, -source, -full_fips_block, -metro_micro_statistical_area_name,-metro_micro_statistical_area_code, -metro_micro_statistical_area_type, -combined_statistical_area_name, -metropolitan_division_area_name, -metropolitan_division_area_code, -addressoccupied)
knitr::opts_chunk$set(echo = TRUE)
options(scipen=999)
##install.packages("ggrepel")
##install.packages('ggthemes')
library(tidyverse)
#Necessary for date parsing.
library(lubridate)
#Necessary for column name cleaning.
library(janitor)
#Eventually necessary for visualization
library(ggthemes)
#Necessary for Census API calls.
library(tidycensus)
#Eventually necessary for visualizations.
library(ggplot2)
#Potentially necessary for cleaning.
library(ggrepel)
#Eventually necessary for visualizations.
library(tigris)
#Simple features.
library(sf)
#Eventually necessary for mapping.
library(ggmap)
# Read in the initial dataset, setting a guess_max limit to improve the accuracy of the data types assigned to each column.
foreclosure_pg <- read_csv("datasets/pg_foreclosures.csv", guess_max = 71676)
#Read in the geocoded address dataset.
foreclosure_pg_census_tracts <- read_csv("datasets/pg_foreclosure_tract_geocodio.csv", guess_max = 71676) |>
clean_names()
# Calculate the number of unique tracts in the geocoded address dataframe.
tracts_unique <- length(unique(foreclosure_pg_census_tracts$census_tract_code))
print(tracts_unique)
# Perform inner join based on the 'location' column.
pg_foreclosures_with_tracts <- inner_join(foreclosure_pg, foreclosure_pg_census_tracts, by = "location")
# Convert "submitteddate" column to date type.
pg_foreclosures_with_tracts$submitteddate <- as.Date(pg_foreclosures_with_tracts$submitteddate, format = "%m/%d/%Y")
# Extract the year and month from "submitteddate".
pg_foreclosures_with_tracts$year <- year(pg_foreclosures_with_tracts$submitteddate)
pg_foreclosures_with_tracts$month <- month(pg_foreclosures_with_tracts$submitteddate)
# Create a year-month column for eventual plotting.
pg_foreclosures_with_tracts$year_month <- as.Date(paste(pg_foreclosures_with_tracts$year, pg_foreclosures_with_tracts$month, "01", sep = "-"), format = "%Y-%m-%d")
# Print the dataset.
print(pg_foreclosures_with_tracts)
# Order the dataframe by the columns that identify remainins duplicate rows.
pg_foreclosures_with_tracts <- pg_foreclosures_with_tracts[order(pg_foreclosures_with_tracts$propertyid, pg_foreclosures_with_tracts$street_address, pg_foreclosures_with_tracts$city.y, pg_foreclosures_with_tracts$submitteddate), ]
# Keep only the last row from each set of duplicates
pg_foreclosures_filtered <- subset(pg_foreclosures_with_tracts, !duplicated(pg_foreclosures_with_tracts[, c("propertyid", "street_address", "city.y", "submitteddate")], fromLast = TRUE))
# Reset row names if necessary
rownames(pg_foreclosures_filtered) <- NULL
# Filter the pg_foreclosures_filtered dataset to remove rows with missing values in the census_tract_code column.
pg_foreclosures_filtered <- pg_foreclosures_filtered[!is.na(pg_foreclosures_filtered$census_tract_code), ]
# Remove unnecessary columns with the select() function.
pg_foreclosures_filtered <- pg_foreclosures_filtered |>
select(pg_foreclosures_filtered, -accuracy_score, -accuracy_type, -source, -full_fips_block, -metro_micro_statistical_area_name,-metro_micro_statistical_area_code, -metro_micro_statistical_area_type, -combined_statistical_area_name, -metropolitan_division_area_name, -metropolitan_division_area_code, -addressoccupied)
View(pg_foreclosures_filtered)
# Remove unnecessary columns with the select() function.
pg_foreclosures_filtered <- pg_foreclosures_filtered |>
select(-accuracy_score, -accuracy_type, -source, -full_fips_block, -metro_micro_statistical_area_name,-metro_micro_statistical_area_code, -metro_micro_statistical_area_type, -combined_statistical_area_name, -metropolitan_division_area_name, -metropolitan_division_area_code, -addressoccupied)
library(dplyr)
# Remove unnecessary columns with the select() function.
pg_foreclosures_filtered <- pg_foreclosures_filtered |>
select(-accuracy_score, -accuracy_type, -source, -full_fips_block, -metro_micro_statistical_area_name,-metro_micro_statistical_area_code, -metro_micro_statistical_area_type, -combined_statistical_area_name, -metropolitan_division_area_name, -metropolitan_division_area_code, -addressoccupied)
knitr::opts_chunk$set(echo = TRUE)
options(scipen=999)
##install.packages("ggrepel")
##install.packages('ggthemes')
library(tidyverse)
library(dplyr)
#Necessary for date parsing.
library(lubridate)
#Necessary for column name cleaning.
library(janitor)
#Eventually necessary for visualization
library(ggthemes)
#Necessary for Census API calls.
library(tidycensus)
#Eventually necessary for visualizations.
library(ggplot2)
#Potentially necessary for cleaning.
library(ggrepel)
#Eventually necessary for visualizations.
library(tigris)
#Simple features.
library(sf)
#Eventually necessary for mapping.
library(ggmap)
# Read in the initial dataset, setting a guess_max limit to improve the accuracy of the data types assigned to each column.
foreclosure_pg <- read_csv("datasets/pg_foreclosures.csv", guess_max = 71676)
#Read in the geocoded address dataset.
foreclosure_pg_census_tracts <- read_csv("datasets/pg_foreclosure_tract_geocodio.csv", guess_max = 71676) |>
clean_names()
# Calculate the number of unique tracts in the geocoded address dataframe.
tracts_unique <- length(unique(foreclosure_pg_census_tracts$census_tract_code))
print(tracts_unique)
# Perform inner join based on the 'location' column.
pg_foreclosures_with_tracts <- inner_join(foreclosure_pg, foreclosure_pg_census_tracts, by = "location")
# Convert "submitteddate" column to date type.
pg_foreclosures_with_tracts$submitteddate <- as.Date(pg_foreclosures_with_tracts$submitteddate, format = "%m/%d/%Y")
# Extract the year and month from "submitteddate".
pg_foreclosures_with_tracts$year <- year(pg_foreclosures_with_tracts$submitteddate)
pg_foreclosures_with_tracts$month <- month(pg_foreclosures_with_tracts$submitteddate)
# Create a year-month column for eventual plotting.
pg_foreclosures_with_tracts$year_month <- as.Date(paste(pg_foreclosures_with_tracts$year, pg_foreclosures_with_tracts$month, "01", sep = "-"), format = "%Y-%m-%d")
# Print the dataset.
print(pg_foreclosures_with_tracts)
# Order the dataframe by the columns that identify remainins duplicate rows.
pg_foreclosures_with_tracts <- pg_foreclosures_with_tracts[order(pg_foreclosures_with_tracts$propertyid, pg_foreclosures_with_tracts$street_address, pg_foreclosures_with_tracts$city.y, pg_foreclosures_with_tracts$submitteddate), ]
# Keep only the last row from each set of duplicates
pg_foreclosures_filtered <- subset(pg_foreclosures_with_tracts, !duplicated(pg_foreclosures_with_tracts[, c("propertyid", "street_address", "city.y", "submitteddate")], fromLast = TRUE))
# Reset row names if necessary
rownames(pg_foreclosures_filtered) <- NULL
# Filter the pg_foreclosures_filtered dataset to remove rows with missing values in the census_tract_code column.
pg_foreclosures_filtered <- pg_foreclosures_filtered[!is.na(pg_foreclosures_filtered$census_tract_code), ]
# Remove unnecessary columns with the select() function.
pg_foreclosures_filtered <- pg_foreclosures_filtered |>
select(-accuracy_score, -accuracy_type, -source, -full_fips_block, -metro_micro_statistical_area_name,-metro_micro_statistical_area_code, -metro_micro_statistical_area_type, -combined_statistical_area_name, -metropolitan_division_area_name, -metropolitan_division_area_code, -addressoccupied)
# Remove unnecessary columns with the select() function.
pg_foreclosures_filtered <- pg_foreclosures_filtered %>
# Remove unnecessary columns with the select() function.
pg_foreclosures_filtered <- pg_foreclosures_filtered |>
select(-accuracy_score, -accuracy_type, -source, -full_fips_block, -metro_micro_statistical_area_name,-metro_micro_statistical_area_code, -metro_micro_statistical_area_type, -combined_statistical_area_name, -metropolitan_division_area_name, -metropolitan_division_area_code, -addressoccupied)
# Remove unnecessary columns with the select() function.
pg_foreclosures_filtered <- pg_foreclosures_filtered %>%
select(-accuracy_score, -accuracy_type, -source, -full_fips_block, -metro_micro_statistical_area_name,-metro_micro_statistical_area_code, -metro_micro_statistical_area_type, -combined_statistical_area_name, -metropolitan_division_area_name, -metropolitan_division_area_code, -addressoccupied)
View(pg_foreclosures_filtered)
# Remove unnecessary columns with the select() function.
pg_foreclosures_filtered <- pg_foreclosures_filtered %>%
select(-accuracy_score, -accuracy_type, -source, -full_fips_block, -metro_micro_statistical_area_name,-metro_micro_statistical_area_code, -metro_micro_statistical_area_type, -combined_statistical_area_name, -metropolitan_division_area_name, -metropolitan_division_area_code, -addressoccupied)
