# Summary of the model including p-values
summary_model <- summary(model_1)
# Calculate p-values
p_values <- drop1(model_1, test = "Chisq")$'Pr(>Chi)'
# Combine coefficients and p-values
coefficients_with_p_values <- cbind(summary_model$coefficients, p_values)
# Print the coefficients with their associated p-values
print(coefficients_with_p_values)
# Extract coefficients
coefficients <- coef(model_1)
# Calculate odds ratios
odds_ratios <- exp(coefficients)
# Calculate log-odds
log_odds <- coefficients
# Print odds ratios and log-odds
print(odds_ratios)
print(log_odds)
knitr::opts_chunk$set(echo = TRUE)
train_data$foreclosure_quantile <- factor(train_data$foreclosure_quantile)
# Use the training dataset for model fitting
model_1 <- polr(foreclosure_quantile ~ mortgaged_2010 + tract_medage_2020 + foreclosure_pc_2010 + pct_built_2000_2009 + poverty_2010 + nhwhite_2020 + mortgage_change_2010_2020 + poverty_change_2010_2020 + nhwhite_change_2010_2020 + medincome_change_2010_2015 + medincome_change_2015_2020 + medincome_change_2010_2020 + pop_change_pct, data = train_data, Hess = TRUE)
# Summarize the model
summary(model_1)
knitr::opts_chunk$set(echo = TRUE)
# Extract coefficients
coefficients <- coef(model_1)
# Extract p-values
p_values <- drop1(model_1, test = "Chisq")$'Pr(>Chi)'
# Calculate odds ratios
odds_ratios <- exp(coefficients)
# Calculate log odds
log_odds <- coefficients
# Print coefficients
print("Coefficients:")
print(coefficients)
# Print p-values
print("P-values:")
print(p_values)
# Print odds ratios
print("Odds Ratios:")
print(odds_ratios)
# Print log odds
print("Log Odds:")
print(log_odds)
knitr::opts_chunk$set(echo = TRUE)
# Extract coefficients
coefficients <- coef(model_1)
# Calculate odds ratios
odds_ratios <- exp(coefficients)
# Calculate log odds
log_odds <- coefficients
# Print coefficients
print("Coefficients:")
print(coefficients)
# Print odds ratios
print("Odds Ratios:")
print(odds_ratios)
# Print log odds
print("Log Odds:")
print(log_odds)
knitr::opts_chunk$set(echo = TRUE)
# Extract coefficients
coefficients <- coef(model_1)
# Calculate odds ratios
odds_ratios <- exp(coefficients)
# Create a data frame with variable names and odds ratios
odds_df <- data.frame(Variable = names(coefficients), Odds_Ratio = odds_ratios)
# Print the organized table
print("Odds Ratios:")
kable(odds_df, format = "markdown", align = c("l", "r"))
knitr::opts_chunk$set(echo = TRUE)
# Extract coefficients
coefficients <- coef(model_1)
# Calculate odds ratios
odds_ratios <- exp(coefficients)
# Print odds ratios
print("Odds Ratios:")
print(odds_ratios)
knitr::opts_chunk$set(echo = TRUE)
train_data$foreclosure_quantile <- factor(train_data$foreclosure_quantile)
# Use the training dataset for model fitting
model_1 <- polr(foreclosure_quantile ~ mortgaged_2010 + tract_medage_2020 + foreclosure_pc_2010 + pct_built_2000_2009 + poverty_2010 + nhwhite_2020 + mortgage_change_2010_2020 + poverty_change_2010_2020 + nhwhite_change_2010_2020 + medincome_change_2010_2015 + medincome_change_2015_2020 + medincome_change_2010_2020 + pop_change_pct, data = train_data, Hess = TRUE)
# Summarize the model
summary(model_1)
knitr::opts_chunk$set(echo = TRUE)
# Convert foreclosure_quantile to factor to make the thing run (not really sure why I need to specify this, but it's the only thing that worked)
train_data$foreclosure_quantile <- factor(train_data$foreclosure_quantile)
# Use the training dataset for model fitting
model_2 <- polr(foreclosure_quantile ~ foreclosure_pc_2010 + tract_medage_2020 + poverty_2010, data = train_data, Hess = TRUE)
# Summarize the model
summary(model_2)
knitr::opts_chunk$set(echo = TRUE)
# Convert foreclosure_quantile to factor
train_data$foreclosure_quantile <- factor(train_data$foreclosure_quantile)
# Use the training dataset for model fitting
model_3 <- polr(foreclosure_quantile ~ foreclosure_pc_2010 + tract_medage_2020 + poverty_2010 + nhwhite_2020 + medincome_change_2010_2020, data = train_data, Hess = TRUE)
# Summarize the model
summary(model_3)
knitr::opts_chunk$set(echo = TRUE)
# Use the training dataset for model fitting
model_4 <- polr(foreclosure_quantile ~ avg_bed + mortgaged_2010 + pct_1_bd, data = train_data, Hess = TRUE)
# Summarize the model
summary(model_4)
knitr::opts_chunk$set(echo = TRUE)
# Use the training dataset for model fitting
model_5 <- polr(foreclosure_quantile ~ avg_bed + mortgaged_2010 + pct_1_bd + pct_built_pre_1960 + pct_built_2000_2009 + mortgage_change_2010_2015, data = train_data, Hess = TRUE)
# Summarize the model
summary(model_5)
knitr::opts_chunk$set(echo = TRUE)
# Use the training dataset for model fitting
model_6 <- polr(foreclosure_quantile ~ foreclosure_pc_2010 + tract_medage_2020 + poverty_2010 + nhwhite_2020 + medincome_change_2010_2020 + avg_bed + mortgaged_2010 + pct_1_bd + pct_built_pre_1960 + pct_built_2000_2009 + mortgage_change_2010_2015 + pop_change_pct, data = train_data, Hess = TRUE)
# Summarize the model
summary(model_6)
knitr::opts_chunk$set(echo = TRUE)
# Define coefficients
coefficients <- coef(model_5)
# Calculate odds ratios
odds_ratios <- exp(coefficients)
# Display the results
result <- data.frame(
Predictor_Variable = names(coefficients),
Odds_Ratio = odds_ratios
)
print(result)
knitr::opts_chunk$set(echo = TRUE)
# Extract coefficients
coefficients <- coef(model_1)
# Calculate odds ratios
odds_ratios <- exp(coefficients)
# Display the results
result <- data.frame(
Predictor_Variable = names(coefficients),
Odds_Ratio = odds_ratios
)
print(result)
knitr::opts_chunk$set(echo = TRUE)
# Predict values using the model and test data
predicted_values <- predict(model_1, newdata = test_data, type = "class")
# Calculate the correlation between predicted and actual values
correlation <- cor(as.numeric(predicted_values), test_data$foreclosure_quantile)
# Plot the correlation using a scatter plot
plot(as.numeric(predicted_values), test_data$foreclosure_quantile,
xlab = "Predicted Values", ylab = "Actual Values",
main = "Correlation between Predicted and Actual Values")
abline(lm(test_data$foreclosure_quantile ~ as.numeric(predicted_values)), col = "red")
text(x = max(as.numeric(predicted_values)),
y = min(test_data$foreclosure_quantile),
labels = paste("Correlation:", round(correlation, 2)), pos = 2)
knitr::opts_chunk$set(echo = TRUE)
# Predict on test data
predictions <- predict(model_1, newdata = test_data, type = "class")
# View the predictions
print(predictions)
# Extract the actual values from test_data
actual_values <- test_data$foreclosure_quantile
# Compare predicted values with actual values
comparison <- data.frame(Actual = actual_values, Predicted = predictions)
# Count the number of matching rows
matching_rows <- sum(comparison$Actual == comparison$Predicted)
# Calculate the total number of rows
total_rows <- nrow(comparison)
# Calculate the percentage of matching rows
matching_percentage <- (matching_rows / total_rows) * 100
# Print the percentage
cat("Percentage of matching rows:", matching_percentage, "%\n")
knitr::opts_chunk$set(echo = TRUE)
# Open a jpeg device to save the plot
jpeg("correlation_plot_log_reg_model_1.jpg")
# Predict values using the model and test data
predicted_values <- predict(model_1, newdata = test_data, type = "class")
# Calculate the correlation between predicted and actual values
correlation <- cor(as.numeric(predicted_values), test_data$foreclosure_quantile)
# Plot the correlation using a scatter plot
plot(as.numeric(predicted_values), test_data$foreclosure_quantile,
xlab = "Predicted Values", ylab = "Actual Values",
main = "Correlation between Predicted and Actual Values")
abline(lm(test_data$foreclosure_quantile ~ as.numeric(predicted_values)), col = "red")
text(x = max(as.numeric(predicted_values)),
y = min(test_data$foreclosure_quantile),
labels = paste("Correlation:", round(correlation, 2)), pos = 2)
# Close the jpeg device to save the plot
dev.off()
knitr::opts_chunk$set(echo = TRUE)
# Open a jpeg device to save the plot
jpeg("correlation_plot_log_reg_model_1.jpg")
# Predict values using the model and test data
predicted_values <- predict(model_1, newdata = test_data, type = "class")
# Calculate the correlation between predicted and actual values
correlation <- cor(as.numeric(predicted_values), test_data$foreclosure_quantile)
# Plot the correlation using a scatter plot
plot(as.numeric(predicted_values), test_data$foreclosure_quantile,
xlab = "Predicted Values", ylab = "Actual Values",
main = "Correlation between Predicted and Actual Values")
abline(lm(test_data$foreclosure_quantile ~ as.numeric(predicted_values)), col = "red")
text(x = max(as.numeric(predicted_values)),
y = min(test_data$foreclosure_quantile),
labels = paste("Correlation:", round(correlation, 2)), pos = 2)
# Close the jpeg device to save the plot
dev.off()
plot.show()
knitr::opts_chunk$set(echo = TRUE)
# Open a jpeg device to save the plot
jpeg("correlation_plot_log_reg_model_1.jpg")
# Predict values using the model and test data
predicted_values <- predict(model_1, newdata = test_data, type = "class")
# Calculate the correlation between predicted and actual values
correlation <- cor(as.numeric(predicted_values), test_data$foreclosure_quantile)
# Plot the correlation using a scatter plot
plot(as.numeric(predicted_values), test_data$foreclosure_quantile,
xlab = "Predicted Values", ylab = "Actual Values",
main = "Correlation between Predicted and Actual Values")
abline(lm(test_data$foreclosure_quantile ~ as.numeric(predicted_values)), col = "red")
text(x = max(as.numeric(predicted_values)),
y = min(test_data$foreclosure_quantile),
labels = paste("Correlation:", round(correlation, 2)), pos = 2)
# Close the jpeg device to save the plot
dev.off()
# Show the plot in Markdown
knitr::include_graphics("correlation_plot.jpg")
knitr::opts_chunk$set(echo = TRUE)
# Open a jpeg device to save the plot
jpeg("correlation_plot_log_reg_model_1.jpg")
# Predict values using the model and test data
predicted_values <- predict(model_1, newdata = test_data, type = "class")
# Calculate the correlation between predicted and actual values
correlation <- cor(as.numeric(predicted_values), test_data$foreclosure_quantile)
# Plot the correlation using a scatter plot
plot(as.numeric(predicted_values), test_data$foreclosure_quantile,
xlab = "Predicted Values", ylab = "Actual Values",
main = "Correlation between Predicted and Actual Values")
abline(lm(test_data$foreclosure_quantile ~ as.numeric(predicted_values)), col = "red")
text(x = max(as.numeric(predicted_values)),
y = min(test_data$foreclosure_quantile),
labels = paste("Correlation:", round(correlation, 2)), pos = 2)
# Close the jpeg device to save the plot
dev.off()
# Show the plot in Markdown
knitr::include_graphics("correlation_plot_log_reg_model_1.jpg")
knitr::opts_chunk$set(echo = TRUE)
# Open a jpeg device to save the plot
jpeg("correlation_plot_log_reg_model_5.jpg")
# Predict values using the model and test data
predicted_values <- predict(model_5, newdata = test_data, type = "class")
# Calculate the correlation between predicted and actual values
correlation <- cor(as.numeric(predicted_values), test_data$foreclosure_quantile)
# Plot the correlation using a scatter plot
plot(as.numeric(predicted_values), test_data$foreclosure_quantile,
xlab = "Predicted Values", ylab = "Actual Values",
main = "Correlation between Predicted and Actual Values")
abline(lm(test_data$foreclosure_quantile ~ as.numeric(predicted_values)), col = "red")
text(x = max(as.numeric(predicted_values)),
y = min(test_data$foreclosure_quantile),
labels = paste("Correlation:", round(correlation, 2)), pos = 2)
# Close the jpeg device to save the plot
dev.off()
# Show the plot in Markdown
knitr::include_graphics("correlation_plot_log_reg_model_5.jpg")
knitr::opts_chunk$set(echo = TRUE)
# Open a jpeg device to save the plot
jpeg("correlation_plot_log_reg_model_6.jpg")
# Predict values using the model and test data
predicted_values <- predict(model_6, newdata = test_data, type = "class")
# Calculate the correlation between predicted and actual values
correlation <- cor(as.numeric(predicted_values), test_data$foreclosure_quantile)
# Plot the correlation using a scatter plot
plot(as.numeric(predicted_values), test_data$foreclosure_quantile,
xlab = "Predicted Values", ylab = "Actual Values",
main = "Correlation between Predicted and Actual Values")
abline(lm(test_data$foreclosure_quantile ~ as.numeric(predicted_values)), col = "red")
text(x = max(as.numeric(predicted_values)),
y = min(test_data$foreclosure_quantile),
labels = paste("Correlation:", round(correlation, 2)), pos = 2)
# Close the jpeg device to save the plot
dev.off()
# Show the plot in Markdown
knitr::include_graphics("correlation_plot_log_reg_model_6.jpg")
knitr::opts_chunk$set(echo = TRUE)
# Define coefficients
coefficients <- coef(model_6)
# Calculate odds ratios
odds_ratios <- exp(coefficients)
# Display the results
result <- data.frame(
Predictor_Variable = names(coefficients),
Odds_Ratio = odds_ratios
)
print(result)
knitr::opts_chunk$set(echo = TRUE)
# Predict on test data
predictions <- predict(model_6, newdata = test_data, type = "class")
# View the predictions
print(predictions)
# Extract the actual values from test_data
actual_values <- test_data$foreclosure_quantile
# Compare predicted values with actual values
comparison <- data.frame(Actual = actual_values, Predicted = predictions)
# Count the number of matching rows
matching_rows <- sum(comparison$Actual == comparison$Predicted)
# Calculate the total number of rows
total_rows <- nrow(comparison)
# Calculate the percentage of matching rows
matching_percentage <- (matching_rows / total_rows) * 100
# Print the percentage
cat("Percentage of matching rows:", matching_percentage, "%\n")
knitr::opts_chunk$set(echo = TRUE)
# Predict on test data
predictions_model_6 <- predict(model_6, newdata = test_data, type = "class")
# View the predictions
print(predictions)
knitr::opts_chunk$set(echo = TRUE)
# Extract the actual values from test_data
actual_values <- test_data$foreclosure_quantile
# Compare predicted values with actual values
comparison <- data.frame(Actual = actual_values, Predicted = predictions_model_6)
# Count the number of matching rows
matching_rows <- sum(comparison$Actual == comparison$Predicted)
# Calculate the total number of rows
total_rows <- nrow(comparison)
# Calculate the percentage of matching rows
matching_percentage <- (matching_rows / total_rows) * 100
# Print the percentage
cat("Percentage of matching rows:", matching_percentage, "%\n")
#Train Naive Bayes Model
# Fit the Naive Bayes model
naive_bayes_model <- naiveBayes(foreclosure_quantile ~ nhwhite_2020 +
foreclosure_pc_2010 +
tract_medage_2020 +
poverty_2010 +
medincome_change_2010_2020 +
avg_bed +
mortgaged_2010 +
pct_1_bd +
pct_built_pre_1960 +
pct_built_2000_2009 +
mortgage_change_2010_2015 +
pop_change_pct, data = pg_foreclosures_per_tract_log_reg)
# Summary of the model
summary(naive_bayes_model)
# Predict using the Naive Bayes model
predictions <- predict(naive_bayes_model, newdata = test_data_2)
# Create the confusion matrix
confusion_matrix <- confusionMatrix(predictions, factor(test_data_2$foreclosure_quantile, levels = 1:5))
# Print the confusion matrix
print(confusion_matrix)
knitr::opts_chunk$set(echo = TRUE)
# Train Naive Bayes Model with Laplace estimator
# Fit the Naive Bayes model with Laplace smoothing
naive_bayes_model_laplace <- naiveBayes(foreclosure_quantile ~ nhwhite_2020 +
foreclosure_pc_2010 +
tract_medage_2020 +
poverty_2010 +
medincome_change_2010_2020 +
avg_bed +
mortgaged_2010 +
pct_1_bd +
pct_built_pre_1960 +
pct_built_2000_2009 +
mortgage_change_2010_2015 +
pop_change_pct,
data = pg_foreclosures_per_tract_log_reg,
laplace = 1)
# Summary of the model with Laplace estimator
summary(naive_bayes_model_laplace)
# Predict using the Naive Bayes model
predictions <- predict(naive_bayes_model_laplace, newdata = test_data_2)
# Create the confusion matrix
confusion_matrix <- confusionMatrix(predictions, factor(test_data_2$foreclosure_quantile, levels = 1:5))
# Print the confusion matrix
print(confusion_matrix)
# Read in the initial dataset, setting a guess_max limit to improve the accuracy of the data types assigned to each column.
foreclosure_pg <- read_csv("datasets/pg_foreclosures.csv", guess_max = 71676)
View(foreclosure_pg)
View(foreclosure_pg)
# Set seed for reproducibility
set.seed(123)
# Split data into training and testing datasets (e.g., 80% training, 20% testing)
train_indices <- createDataPartition(pg_foreclosures_per_tract$foreclosure_pc_2020, p = 0.7, list = FALSE)
pg_foreclosures_train <- pg_foreclosures_per_tract[train_indices, ]
pg_foreclosures_test <- pg_foreclosures_per_tract[-train_indices, ]
for (variable in variables_of_interest) {
# Perform linear regression
model <- lm(foreclosure_pc_2020 ~ ., data = pg_foreclosures_train[, c(variable, "foreclosure_pc_2020")])
# Extract required information
intercept <- coef(model)[1]
coefficient <- coef(model)[2]
# Check if the coefficient is statistically significant
p_value <- summary(model)$coefficients[which(rownames(summary(model)$coefficients) == variable), "Pr(>|t|)"]
is_predictive <- ifelse(p_value < 0.05, "Yes", "No")
# Compute residuals
residuals <- resid(model)
# Compute correlation between predicted and real values
correlation <- cor(predict(model), pg_foreclosures_train$foreclosure_pc_2020 )
# Compute mean square error
mse <- mean((predict(model) - pg_foreclosures_train$foreclosure_pc_2020 )^2)
# Print results
cat("Variable:", variable, "\n")
cat("Intercept:", intercept, "\n")
cat("Coefficient:", coefficient, "\n")
cat("Is it a predictive feature?:", is_predictive, "\n")
cat("Correlation:", correlation, "\n")
cat("Mean Square Error:", mse, "\n")
cat("\n")
}
# Selecting the variables of interest (excluding foreclosure_pc_2020)
variables_of_interest <- c("avg_bed", "tract_homevalue_2020", "tract_medage_2020", "tract_medincome_2020",
"tract_medincome_2010", "foreclosure_pc_2010",
"pct_built_2020_later", "pct_built_2010_2019", "pct_built_2000_2009",
"pct_built_1990_1999", "pct_built_1980_1989", "pct_built_1970_1979",
"pct_built_pre_1960", "pct_0_bd", "pct_1_bd", "pct_2_bd", "pct_3_bd",
"pct_4_more_bd", "poverty_2010", "poverty_2020", "nhwhite_2010", "nhwhite_2020",
"mortgaged_2010", "mortgaged_2015", "mortgaged_2020", "ownoccupied_2010",
"ownoccupied_2015", "ownoccupied_2020", "mortgage_change_2010_2015",
"mortgage_change_2015_2020", "mortgage_change_2010_2020","ownoccupied_change_2010_2015",                             "ownoccupied_change_2015_2020", "ownoccupied_change_2010_2020",
"poverty_change_2010_2020","nhwhite_change_2010_2020","medincome_change_2010_2015",
"medincome_change_2015_2020", "medincome_change_2010_2020", "pop_change_pct")
for (variable in variables_of_interest) {
# Perform linear regression
model <- lm(foreclosure_pc_2020 ~ ., data = pg_foreclosures_train[, c(variable, "foreclosure_pc_2020")])
# Extract required information
intercept <- coef(model)[1]
coefficient <- coef(model)[2]
# Check if the coefficient is statistically significant
p_value <- summary(model)$coefficients[which(rownames(summary(model)$coefficients) == variable), "Pr(>|t|)"]
is_predictive <- ifelse(p_value < 0.05, "Yes", "No")
# Compute residuals
residuals <- resid(model)
# Compute correlation between predicted and real values
correlation <- cor(predict(model), pg_foreclosures_train$foreclosure_pc_2020 )
# Compute mean square error
mse <- mean((predict(model) - pg_foreclosures_train$foreclosure_pc_2020 )^2)
# Print results
cat("Variable:", variable, "\n")
cat("Intercept:", intercept, "\n")
cat("Coefficient:", coefficient, "\n")
cat("Is it a predictive feature?:", is_predictive, "\n")
cat("Correlation:", correlation, "\n")
cat("Mean Square Error:", mse, "\n")
cat("\n")
}
variables_of_interest_subset_2 <- c(
"pct_built_2020_later", "pct_built_2010_2019", "pct_built_2000_2009",
"pct_built_1990_1999", "pct_built_1980_1989", "pct_built_1970_1979",
"pct_built_pre_1960", "pct_0_bd", "pct_1_bd", "pct_2_bd", "pct_3_bd",
"pct_4_more_bd")
# Set the lambda (regularization) parameter
lambda <- 0.2
# Initialize vectors to store correlation coefficients and mean squared errors
correlation_coefficients <- numeric(length(variables_of_interest_subset_1))
knitr::opts_chunk$set(echo = TRUE)
options(scipen=999)
##install.packages("ggrepel")
##install.packages('ggthemes')
##install.packages('gplots')
#install.packages('car')
##install.packages("glmnet")
library(tidyverse)
library(dplyr)
#Necessary for date parsing.
library(lubridate)
#Necessary for column name cleaning.
library(janitor)
#Eventually necessary for visualization
library(ggthemes)
#Necessary for Census API calls.
library(tidycensus)
#Eventually necessary for visualizations.
library(ggplot2)
library(gplots)
#Potentially necessary for cleaning.
library(ggrepel)
#Eventually necessary for visualizations.
library(tigris)
#Simple features.
library(sf)
#Eventually necessary for mapping.
library(ggmap)
#Needed for dividing into training and test sets.
library(caret)
#Needed for identifying collinear variables.
library(car)
#Needed for regularization
library(glmnet)
# Read in the initial dataset, setting a guess_max limit to improve the accuracy of the data types assigned to each column.
foreclosure_pg <- read_csv("datasets/pg_foreclosures.csv", guess_max = 71676)
#Read in the geocoded address dataset.
foreclosure_pg_census_tracts <- read_csv("datasets/pg_foreclosure_tract_geocodio.csv", guess_max = 71676) |>
clean_names()
# Calculate the number of unique tracts in the geocoded address dataframe.
tracts_unique <- length(unique(foreclosure_pg_census_tracts$census_tract_code))
print(tracts_unique)
# Perform inner join based on the 'location' column.
pg_foreclosures_with_tracts <- inner_join(foreclosure_pg, foreclosure_pg_census_tracts, by = "location")
# Convert "submitteddate" column to date type.
pg_foreclosures_with_tracts$submitteddate <- as.Date(pg_foreclosures_with_tracts$submitteddate, format = "%m/%d/%Y")
# Extract the year and month from "submitteddate".
pg_foreclosures_with_tracts$year <- year(pg_foreclosures_with_tracts$submitteddate)
pg_foreclosures_with_tracts$month <- month(pg_foreclosures_with_tracts$submitteddate)
# Create a year-month column for eventual plotting.
pg_foreclosures_with_tracts$year_month <- as.Date(paste(pg_foreclosures_with_tracts$year, pg_foreclosures_with_tracts$month, "01", sep = "-"), format = "%Y-%m-%d")
# Print the dataset.
print(pg_foreclosures_with_tracts)
# Order the dataframe by the columns that identify remainins duplicate rows.
pg_foreclosures_with_tracts <- pg_foreclosures_with_tracts[order(pg_foreclosures_with_tracts$propertyid, pg_foreclosures_with_tracts$street_address, pg_foreclosures_with_tracts$city.y, pg_foreclosures_with_tracts$submitteddate), ]
# Keep only the last row from each set of duplicates
pg_foreclosures_filtered <- subset(pg_foreclosures_with_tracts, !duplicated(pg_foreclosures_with_tracts[, c("propertyid", "street_address", "city.y", "submitteddate")], fromLast = TRUE))
# Reset row names if necessary
rownames(pg_foreclosures_filtered) <- NULL
# Filter the pg_foreclosures_filtered dataset to remove rows with missing values in the census_tract_code column.
pg_foreclosures_filtered <- pg_foreclosures_filtered[!is.na(pg_foreclosures_filtered$census_tract_code), ]
# Remove unnecessary columns with the select() function.
pg_foreclosures_filtered <- pg_foreclosures_filtered %>%
select(-accuracy_score, -accuracy_type, -source, -full_fips_block, -metro_micro_statistical_area_name,-metro_micro_statistical_area_code, -metro_micro_statistical_area_type, -combined_statistical_area_name, -metropolitan_division_area_name, -metropolitan_division_area_code, -addressoccupied)
