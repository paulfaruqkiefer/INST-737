labs(x = "Month", y = "Intent to Foreclose Notices", title = "Monthly Intent to Foreclose Notices: Whiteplains, MD") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
whiteplains_monthly_foreclosure_chart
monthly_sum_long|>
# group by parsed month
group_by(month_parsed) |>
# sum up foreclosure totals by months over the years
summarize(
total_intent_to_foreclose_notices = sum(monthly_foreclosure_total)
) |>
# arrange from highest to lowest
arrange(desc(total_intent_to_foreclose_notices))
# for fiscal year 2021
monthly_sum_long_fy21 <- monthly_sum_long |>
filter(month >= "2021-07-01" & month <= "2022-06-01")
# create bar graph
ggplot(monthly_sum_long_fy21, aes(x = month, y = monthly_foreclosure_total)) +
geom_bar(stat = "identity", fill = "skyblue") +
labs(x = "Month", y = "Total Foreclosure Notices", title = "Monthly Foreclosure Notice Totals in FY 2021") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# for fiscal year 2022
monthly_sum_long_fy22 <- monthly_sum_long |>
filter(month >= "2022-07-01" & month <= "2023-06-01")
# create bar graph
ggplot(monthly_sum_long_fy22, aes(x = month, y = monthly_foreclosure_total)) +
geom_bar(stat = "identity", fill = "skyblue") +
labs(x = "Month", y = "Total Foreclosure Notices", title = "Monthly Foreclosure Notice Totals in FY 2021") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
monthly_sum_long |>
# filter for only fy 21-22
filter(month >= "2021-07-01" & month <= "2023-06-01") |>
# group by parsed month
group_by(month_parsed) |>
# sum up foreclosure totals by months over the years
summarize(
total_intent_to_foreclose_notices = sum(monthly_foreclosure_total)
) |>
# arrange from highest to lowest
arrange(desc(total_intent_to_foreclose_notices))
md_foreclosure_with_zip_pop_housing_income_2011 <- left_join(md_foreclosure_with_zip_pop_and_housing, md_medhouseholdincome_2011_zip, by=c("zip" = "NAME")) |> rename(med_householdincome_2011_zip = estimate)
md_foreclosure_with_zip_pop_housing_income <- left_join(md_foreclosure_with_zip_pop_housing_income_2011, md_medhouseholdincome_2021_zip, by=c("zip" = "GEOID")) |> rename(med_householdincome_2021_zip = estimate)
md_foreclosure_with_zip_pop_housing_income <- md_foreclosure_with_zip_pop_housing_income |>
select(-variable.x, -NAME, -variable.y)
md_foreclosure_with_zip_pop_housing_income <- md_foreclosure_with_zip_pop_housing_income |>
mutate(income_change_2011_2021= ((med_householdincome_2021_zip - med_householdincome_2011_zip)/med_householdincome_2011_zip * 100))
na_zip_income_change <- md_foreclosure_with_zip_pop_housing_income|>
filter(is.na(income_change_2011_2021))
na_zip_income_change
md_foreclosure_with_zip_pop_housing_income <- md_foreclosure_with_zip_pop_housing_income |>
filter(!is.na(income_change_2011_2021)) |>
arrange(income_change_2011_2021)
md_foreclosure_with_zip_pop_housing_income
md_foreclosure_with_zip_pop_housing_income |>
arrange(desc(foreclosure_pc))
correlation_income_change_and_foreclosure_pc <- cor(md_foreclosure_with_zip_pop_housing_income$income_change_2011_2021,
md_foreclosure_with_zip_pop_housing_income$foreclosure_pc)
correlation_income_change_and_foreclosure_pc
ggplot() +
geom_sf(data=md_foreclosure_with_zip_pop_housing_income, aes(geometry= geometry, fill=income_change_2011_2021)) +
scale_fill_gradientn(
colors = c("lightgreen","darkorange", "#A84268"))+
theme_minimal()
ggplot(md_foreclosure_with_zip_pop_housing_income, aes(x = income_change_2011_2021, y = foreclosure_pc, label = zip)) +
geom_point() +
geom_text_repel(box.padding = 0.2) +  # Adjust box.padding as needed for label positioning
labs(title = "No clear impact of income change from 2011 to 2021 and intent to foreclose notices per capita",
x = "Median household income change from 2011 to 2021",
y = "Intent to foreclose notices per 1,000 residents")
# Join 2014 median household income data with the intent to foreclose notice dataset.
md_foreclosure_with_zip_pop_housing_income <- left_join(md_foreclosure_with_zip_pop_housing_income, md_medhouseholdincome_2014_zip, by=c("zip" = "GEOID")) |> rename(med_householdincome_2014_zip = estimate)
# Create a new column to calculate median household income change from 2014-2021 for each zip code.
md_foreclosure_with_zip_pop_housing_income <- md_foreclosure_with_zip_pop_housing_income |>
mutate(income_change_2014_2021 = ((med_householdincome_2021_zip - med_householdincome_2014_zip)/med_householdincome_2014_zip * 100))
# Filter for NA values in household income change column.
na_zip_income_change_2014_2021 <- md_foreclosure_with_zip_pop_housing_income|>
filter(is.na(income_change_2014_2021))
# Use anti-join to identify zip codes with NA values that did not appear in the 2011-2021 comparison.
missing_income_change_2014_2021 <- anti_join(na_zip_income_change_2014_2021, na_zip_income_change, by = "zip")
missing_income_change_2014_2021
# Remove rows with NA values in the median household income change 2014-2021 column.
md_foreclosure_with_zip_pop_housing_income <- md_foreclosure_with_zip_pop_housing_income |>
filter(!is.na(income_change_2014_2021)) |>
arrange(income_change_2014_2021)
# Calculate correlation coefficient.
correlation_income_change_and_foreclosure_pc_2014_2021 <- cor(md_foreclosure_with_zip_pop_housing_income$income_change_2014_2021,
md_foreclosure_with_zip_pop_housing_income$foreclosure_pc)
correlation_income_change_and_foreclosure_pc_2014_2021
# Join 2017 median household income data with the intent to foreclose notice dataset.
md_foreclosure_with_zip_pop_housing_income <- left_join(md_foreclosure_with_zip_pop_housing_income, md_medhouseholdincome_2017_zip, by=c("zip" = "GEOID")) |> rename(med_householdincome_2017_zip = estimate)
# Create a new column to calculate median household income change from 2017-2021 for each zip code.
md_foreclosure_with_zip_pop_housing_income <- md_foreclosure_with_zip_pop_housing_income |>
mutate(income_change_2017_2021 = ((med_householdincome_2021_zip - med_householdincome_2017_zip)/med_householdincome_2017_zip * 100))
# Filter for NA values in household income change column.
na_zip_income_change_2017_2021 <- md_foreclosure_with_zip_pop_housing_income|>
filter(is.na(income_change_2017_2021))
# Use anti-join to identify zip codes with NA values that did not appear in the 2011-2021 comparison.
missing_income_change_2017_2021 <- anti_join(na_zip_income_change_2017_2021, na_zip_income_change, by = "zip")
missing_income_change_2017_2021
# Remove rows with NA values in the median household income change 2017-2021 column.
md_foreclosure_with_zip_pop_housing_income <- md_foreclosure_with_zip_pop_housing_income |>
filter(!is.na(income_change_2017_2021)) |>
arrange(income_change_2017_2021)
# Calculate correlation coefficient.
correlation_income_change_and_foreclosure_pc_2017_2021 <- cor(md_foreclosure_with_zip_pop_housing_income$income_change_2017_2021,
md_foreclosure_with_zip_pop_housing_income$foreclosure_pc)
correlation_income_change_and_foreclosure_pc_2017_2021
# Calculate correlation coefficient.
correlation_income_foreclosure_pc <- cor(md_foreclosure_with_zip_pop_housing_income$med_householdincome_2021_zip,
md_foreclosure_with_zip_pop_housing_income$foreclosure_pc)
correlation_income_foreclosure_pc
md_foreclosure_with_zip_pop_and_housing_age <- left_join(md_foreclosure_with_zip_pop_and_housing, md_medage, by=c("zip" = "GEOID")) |> rename(median_age = estimate)
na_zip_median_age <- md_foreclosure_with_zip_pop_and_housing_age |>
filter(is.na(median_age))
na_zip_median_age
# Remove rows with NA values in the median age column.
md_foreclosure_with_zip_pop_and_housing_age <- md_foreclosure_with_zip_pop_and_housing_age |>
filter(!is.na(median_age))
# Calculate correlation coefficient.
correlation_median_income_foreclosure_notices_pc <- cor(md_foreclosure_with_zip_pop_and_housing_age$median_age,
md_foreclosure_with_zip_pop_and_housing_age$foreclosure_pc)
correlation_median_income_foreclosure_notices_pc
md_foreclosure_with_zip_pop_and_housing <- md_foreclosure_with_zip_pop_and_housing |>
mutate(pct_owner_occupied = (total_owner_units/total_housing_units)*100,
pct_debt_units = (total_debt_units/total_housing_units)*100)
md_foreclosure_with_zip_pop_and_no_housing <- md_foreclosure_with_zip_pop_and_housing |>
filter(total_housing_units == 0)
md_foreclosure_with_zip_pop_and_no_housing
md_foreclosure_with_zip_pop_and_housing <- md_foreclosure_with_zip_pop_and_housing |>
filter(total_housing_units != 0)
correlation_owner_occupied_foreclosure_pc <- cor(md_foreclosure_with_zip_pop_and_housing$pct_owner_occupied,
md_foreclosure_with_zip_pop_and_housing$foreclosure_pc)
correlation_owner_occupied_foreclosure_pc
correlation_debt_units_foreclosure_pc <- cor(md_foreclosure_with_zip_pop_and_housing$pct_debt_units,
md_foreclosure_with_zip_pop_and_housing$foreclosure_pc)
correlation_debt_units_foreclosure_pc
mean_assessed_property_value_zip_code <- read_csv("foreclosure_dataset/zip_means.csv", guess_max = 4497)
md_foreclosure_with_assessed_property_value <- left_join(md_foreclosure_with_zip_pop, mean_assessed_property_value_zip_code, by=c("zip" = "zipcode")) |>
filter(!is.na(mean_assessment))
correlation_mean_assessed_property_value_and_foreclosure_pc <- cor(md_foreclosure_with_assessed_property_value$mean_assessment,
md_foreclosure_with_assessed_property_value$foreclosure_pc)
correlation_mean_assessed_property_value_and_foreclosure_pc
monthly_foreclosure_totals_bar
md_notice_dif_state_mean_log_plot
crapo_monthly_foreclosure_chart
whiteplains_monthly_foreclosure_chart
setwd("C:/Users/paulf/GitHub/congressional-private-travel/house_travel")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(dplyr)
library(ggplot2)
stand_dests_df <- read_csv("archive/standardize_sponsors_dests/standardizing_dests/house_12to23_w_stand_dests.csv")
stand_dests_df
sponsor_list <- read_delim("archive/standardize_sponsors_dests/standardized_sponsors/open_refined_sponsors.csv")
sponsor_list
house_travel <- stand_dests_df %>%
full_join(sponsor_list)
# Convert "ReturnDate" to Date type
house_travel$ReturnDate <- as.Date(house_travel$ReturnDate, format = "%m/%d/%Y")
# Extract the year from "DepartureDate" and populate "Year" column
house_travel$Year <- year(house_travel$DepartureDate)
# Extract the year/month combination from "DepartureDate" and populate "Year" column
house_travel$year_month <- paste(year(house_travel$DepartureDate), month(house_travel$DepartureDate, label = TRUE), sep = "-")
# Filter out non-members or staffers
house_travel <- house_travel %>%
filter(!is.na(State) & !is.na(District))
# Print or use house_travel as needed
print(house_travel)
# Identify all pairs in the house_travel dataset that have matching DocIDs and save those with matching DocIDs for further review.
duplicate_docids_house_travel <- house_travel %>%
group_by(DocID) %>%
filter(n() > 1)
combined_duplicate_docids <- duplicate_docids_house_travel %>%
group_by(DocID) %>%
summarise(
across(
-c(DepartureDate, ReturnDate),
~ ifelse(
length(unique(na.omit(.))) > 1,
paste(na.omit(.), collapse = "; "),
first(na.omit(.))
)
),
DepartureDate = min(DepartureDate, na.rm = TRUE), #This is an imperfect method of combining travel date information, but because these forms have identical date information, it works.
ReturnDate = max(ReturnDate, na.rm = TRUE)
)
filtered_house_travel <- house_travel %>%
anti_join(duplicate_docids_house_travel, by = "DocID")
# Identify duplicate rows based on the combination of values in specified columns that remain consistent from form to form.
# To cast a wide net, we create one version that identifies rows with matching values in the FilerName, MemberName, State, Year and standardized_dest columns -- columns that consistently match from entry to entry.
columns_wide <- c("FilerName", "MemberName", "State", "Year", "standardized_dest")
# To cast a narrower net, we create another version that identifies rows with matching values in the FilerName, MemberName, State, and standardized_dest columns, as well as the year_month and sponsors_cleaned columns -- columns that may change within an otherwise matching set.
columns_narrow <- c("FilerName", "MemberName", "State", "year_month", "standardized_dest", "sponsors_cleaned")
duplicates_logical_wide <- duplicated(filtered_house_travel[columns_wide], fromLast = TRUE) | duplicated(filtered_house_travel[columns_wide])
duplicates_logical_narrow <- duplicated(filtered_house_travel[columns_narrow], fromLast = TRUE) | duplicated(filtered_house_travel[columns_narrow])
# Subset the original dataset using the wide and narrow duplicate vectors to get the duplicate rows
duplicates_wide <- subset(filtered_house_travel, duplicates_logical_wide)
duplicates_narrow <- subset(filtered_house_travel, duplicates_logical_narrow)
# Arrange the rows to consolidate matching groups.
duplicates_wide <- arrange(duplicates_wide, FilerName, MemberName, State, Year, standardized_dest)
duplicates_narrow <- arrange(duplicates_narrow, FilerName, MemberName, State, Year, standardized_dest, sponsors_cleaned)
missing <- anti_join(duplicates_narrow, duplicates_wide, by = c("DocID"))
# Use the DocID column to identify the most recent version of a travel filing.
rows_to_keep <- duplicates_narrow %>%
group_by(FilerName, MemberName, State, year_month, standardized_dest, sponsors_cleaned) %>%
filter(DocID == max(DocID))
diff_sponsor_month <- anti_join(duplicates_wide, duplicates_narrow, by = c("FilerName", "MemberName", "State", "year_month", "standardized_dest", "sponsors_cleaned"))
no_change_diff_sponsor_month_rows <- diff_sponsor_month %>%
group_by(across(-c(TravelSponsor, sponsors_cleaned, DocID, index_number, FilingType, Destination))) %>%
filter(n() == 1) %>%
ungroup()
consolidated_diff_sponsor_rows <- diff_sponsor_month %>%
group_by(across(-c(TravelSponsor, sponsors_cleaned, DocID, index_number, FilingType, Destination))) %>%
filter(n() > 1) %>%
slice_max(order_by = DocID) %>%
ungroup()
filtered_house_travel <- anti_join(filtered_house_travel, duplicates_wide, by = c("DocID"))
# Convert 'index_number' to numeric in combined_duplicate_docids
combined_duplicate_docids <- mutate(combined_duplicate_docids, index_number = as.numeric(index_number))
# Now you can bind the rows
filtered_house_travel <- bind_rows(filtered_house_travel, rows_to_keep)
filtered_house_travel <- bind_rows(filtered_house_travel, combined_duplicate_docids)
filtered_house_travel <- bind_rows(filtered_house_travel, consolidated_diff_sponsor_rows)
# Group by the specified columns and count the occurrences of each combination
grouped_same_dest_diff_spons <- no_change_diff_sponsor_month_rows %>%
group_by(MemberName, FilerName, State, standardized_dest, year_month) %>%
summarise(count = n()) %>%
filter(count > 1)
# Filter for rows where count is greater than 1, meaning they have duplicates
same_dest_diff_spons <- no_change_diff_sponsor_month_rows %>%
inner_join(grouped_same_dest_diff_spons, by = c('MemberName', 'FilerName', 'State', 'standardized_dest', 'year_month'))
# Group by the specified columns and count the occurrences of each combination
grouped_same_dest_diff_month <- no_change_diff_sponsor_month_rows %>%
group_by(MemberName, FilerName, State, standardized_dest, sponsors_cleaned) %>%
summarise(count = n()) %>%
filter(count > 1)
# Filter for rows where count is greater than 1, meaning they have duplicates
same_dest_diff_month <- no_change_diff_sponsor_month_rows %>%
inner_join(grouped_same_dest_diff_month, by = c('MemberName', 'FilerName', 'State', 'standardized_dest', 'sponsors_cleaned'))
no_change_diff_sponsor_month_rows <- subset(no_change_diff_sponsor_month_rows,
!(MemberName == "Beyer, Donald" &
FilingType == "Original" &
DepartureDate == "2021-11-19" &
sponsors_cleaned == "United Kingdom Marshall Scholars"))
possible_duplicates <- subset(no_change_diff_sponsor_month_rows,
sponsors_cleaned %in% c("Us-qatar Business Council",
"Hillsdale College",
"Washington Office On Latin America",
"Japan Center For International Exchange"))
filtered_no_change_diff_sponsor_month_rows <- anti_join(no_change_diff_sponsor_month_rows, possible_duplicates, by = c("DocID"))
filtered_house_travel <- bind_rows(filtered_house_travel, filtered_no_change_diff_sponsor_month_rows)
# Write the filtered house travel dataset to a CSV file
write_csv(filtered_house_travel, "deduped_house_travel_full.csv")
# Write the remaining possible duplicates dataset to a CSV file.
write_csv(possible_duplicates, "possible_duplicates.csv")
filtered_house_travel %>%
filter(Year > 2020) |>
group_by(sponsors_cleaned) %>%
summarise(count = n()) %>%
arrange(desc(count))
biofuels_filtered <- filtered_house_travel[grepl("Renewable Fuels|Clean Fuels|Biodiesel|Biofuel", filtered_house_travel$sponsors_cleaned, ignore.case = TRUE), ]
biofuels_filtered |>
filter(Year > 2020) |>
arrange(desc(DepartureDate))
sugar_filtered <- filtered_house_travel[grepl("Sugar|Sugarbeet|Sugarcane|South Florida Agricultural|Agricultural Institute of Florida|Louisiana Farm Bureau|Leadership Idaho Agriculture", filtered_house_travel$sponsors_cleaned, ignore.case = TRUE), ]
sugar_filtered |>
arrange(desc(DepartureDate))
sugar_filtered |>
group_by(sponsors_cleaned) |>
summarize(number_trips = n()) |>
arrange(desc(number_trips))
sugar_filtered |>
filter(Year>2014 & Year<2018) |>
group_by(MemberName) |>
summarize(number_trips = n()) |>
arrange(desc(number_trips))
sugar_filtered |>
group_by(State) |>
summarize(number_trips = n()) |>
arrange(desc(number_trips))
# Convert the "Date" column to Date type if it's not already
sugar_filtered$Date <- as.Date(sugar_filtered$DepartureDate)
# Create a line chart
sugar_filtered_plot <- ggplot(sugar_filtered, aes(x = DepartureDate)) +
geom_line(stat = "count", color = "blue") +
labs(title = "Number of Trips Sponsored in Whole or in Part by Sugar Interests",
x = "Date",
y = "Trips") +
theme_minimal()
sugar_filtered_plot
sugar_filtered <- sugar_filtered %>%
mutate(Month = format(DepartureDate, "%Y-%m"))
sugar_dests_by_month_filtered <- sugar_filtered %>%
group_by(Month, standardized_dest) %>%
summarise(count = n()) %>%
arrange(desc(count))
top5_per_month_sugar_filtered <- sugar_dests_by_month_filtered %>%
group_by(Month) %>%
top_n(5, count)
top5_per_month_sugar_filtered |>
arrange(desc(Month))
# Filter rows to isolate trips sponsored in whole or in part by other agricultural interests.
other_ag <- filtered_house_travel[grepl("St. Louis Agribusiness Club|Farm Journal Foundation|National Association of State Departments of Agriculture|Farm Credit of the Virginias|Bowery Farming|Massachusetts Farm Bureau|National Farmers Union|National Grain and Feed Association|North Carolina Vegetable Growers Association|Livestock Marketing Association|Farm Foundation", filtered_house_travel$sponsors_cleaned, ignore.case = TRUE), ]
other_ag|>
arrange(desc(DepartureDate))
# Filter rows to isolate trips sponsored by conservation or clean energy organizations that may have an interest in the sugar industry or its environmental impacts.
conservation_sugar_regions_narrow <- house_travel[grepl("The Everglades Foundation|National Parks Conservation Association|Center for Clean Air Policy|Coalition to Restore Coastal Louisiana|Idaho Conservation League", house_travel$sponsors_cleaned, ignore.case = TRUE), ]
conservation_sugar_regions_narrow|>
arrange(desc(DepartureDate))
louisiana_sugar <- subset(sugar_filtered, grepl("Louisiana Sugar", TravelSponsor, ignore.case = TRUE))
# Display the resulting subset
print(louisiana_sugar)
louisiana_sugar |>
group_by(Year) |>
summarize(trip_count = n()) |>
arrange(desc(trip_count))
louisiana_sugar |>
filter(Year>2014 & Year<2018) |>
group_by(MemberName, Year) |>
summarize(trip_count = n()) |>
arrange(desc(Year))
louisiana_sugar |>
group_by(State) |>
summarize(trip_count = n()) |>
arrange(desc(trip_count))
midwest_sugar <- sugar_filtered[grepl("Sugarbeet|Leadership Idaho Agriculture", sugar_filtered$sponsors_cleaned, ignore.case = TRUE), ]
florida_sugar <- sugar_filtered[grepl("Florida", sugar_filtered$sponsors_cleaned, ignore.case = TRUE), ]
ag_inst_florida_sugar <- sugar_filtered[grepl("Agricultural Institute Of Florida Foundation", sugar_filtered$sponsors_cleaned, ignore.case = TRUE), ]
midwest_sugar |>
group_by(Year) |>
summarize(trip_count = n()) |>
arrange(desc(trip_count))
sugar_filtered |>
filter(MemberName == "Peterson, Collin C.") |>
group_by(FilerName) |>
summarize(number_trips = n()) |>
arrange(desc(number_trips))
sugar_filtered |>
filter(MemberName == "Yoho, Ted S.") |>
group_by(sponsors_cleaned) |>
summarize(number_trips = n()) |>
arrange(desc(number_trips))
bifouels_filtered |>
group_by(MemberName) |>
summarize(number_trips = n()) |>
arrange(desc(number_trips))
biofuels_filtered <- filtered_house_travel[grepl("Renewable Fuels|Clean Fuels|Biodiesel|Biofuel", filtered_house_travel$sponsors_cleaned, ignore.case = TRUE), ]
biofuels_filtered |>
filter(Year > 2020) |>
arrange(desc(DepartureDate))
bifouels_filtered |>
group_by(MemberName) |>
summarize(number_trips = n()) |>
arrange(desc(number_trips))
setwd("C:/Users/paulf/GitHub/congressional-private-travel/house_travel")
biofuels_filtered |>
group_by(MemberName) |>
summarize(number_trips = n()) |>
arrange(desc(number_trips))
sugar_filtered |>
filter(MemberName == "Yoho, Ted") |>
group_by(FilerName) |>
summarize(number_trips = n()) |>
arrange(desc(number_trips))
View(sugar_filtered)
sugar_filtered |>
filter(MemberName == "Yoho, Ted S.") |>
group_by(FilerName) |>
summarize(number_trips = n()) |>
arrange(desc(number_trips))
sugar_filtered |>
group_by(MemberName) |>
summarize(number_trips = n()) |>
arrange(desc(number_trips))
sugar_filtered |>
filter(MemberName == "Peterson, Collin C.") |>
group_by(FilerName) |>
summarize(number_trips = n()) |>
arrange(desc(number_trips))
sugar_filtered |>
filter(MemberName == "Scott, Austin") |>
group_by(FilerName) |>
summarize(number_trips = n()) |>
arrange(desc(number_trips))
sugar_filtered |>
filter(MemberName == "Yoho, Ted S.") |>
group_by(FilerName) |>
summarize(number_trips = n()) |>
arrange(desc(number_trips))
# Filter rows to isolate trips sponsored by conservation or clean energy organizations that may have an interest in the sugar industry or its environmental impacts.
conservation_sugar_regions_narrow <- house_travel[grepl("The Everglades Foundation|National Parks Conservation Association|Center for Clean Air Policy|Coalition to Restore Coastal Louisiana|Idaho Conservation League", house_travel$sponsors_cleaned, ignore.case = TRUE), ]
conservation_sugar_regions_narrow|>
group_by(MemberName) |>
summarize(count = n()) |>
arrange(desc(count))
sugar_filtered |>
filter(MemberName == "Peterson, Collin C.") |>
group_by(Year) |>
summarize(number_trips = n()) |>
arrange(desc(number_trips))
sugar_filtered |>
filter(MemberName == "Peterson, Collin C.") |>
group_by(FilerName) |>
summarize(number_trips = n()) |>
arrange(desc(number_trips))
sugar_filtered |>
filter(MemberName == "Yoho, Ted S.") |>
group_by(FilerName) |>
summarize(number_trips = n()) |>
arrange(desc(number_trips))
sugar_filtered |>
filter(MemberName == "Scott, Austin") |>
group_by(FilerName) |>
summarize(number_trips = n()) |>
arrange(desc(number_trips))
sugar_filtered |>
filter(MemberName == "Diaz-Balart, Mario") |>
group_by(FilerName) |>
summarize(number_trips = n()) |>
arrange(desc(number_trips))
sugar_filtered |>
filter(MemberName == "Lawson, Al") |>
group_by(FilerName) |>
summarize(number_trips = n()) |>
arrange(desc(number_trips))
sugar_filtered |>
filter(MemberName == "Allen, Rick") |>
group_by(FilerName) |>
summarize(number_trips = n()) |>
arrange(desc(number_trips))
sugar_filtered |>
filter(MemberName == "Bacon, Don") |>
group_by(FilerName) |>
summarize(number_trips = n()) |>
arrange(desc(number_trips))
setwd("C:/Users/paulf/GitHub/INST-737")
knitr::opts_chunk$set(echo = TRUE)
options(scipen=999)
#install.packages("ggrepel")
#install.packages('ggthemes')
library(tidyverse)
library(lubridate)
library(janitor)
library(ggthemes)
library(tidycensus)
library(ggplot2)
library(ggrepel)
library(tigris)
library(sf)
foreclosure_pg <- read_csv("datasets/pg_foreclosures.csv", guess_max = 71677)
View(foreclosure_pg)
pg_foreclosures <- na.omit(pg_foreclosures[!is.na(pg_foreclosures$zip_code), ])
pg_foreclosures_filtered <- na.omit(foreclosure_pg[!is.na(foreclosure_pg$zip_code), ])
View(pg_foreclosures_filtered)
knitr::opts_chunk$set(echo = TRUE)
options(scipen=999)
install.packages("ggrepel")
install.packages('ggthemes')
devtools::install_github("hrbrmstr/rgeocodio")
knitr::opts_chunk$set(echo = TRUE)
options(scipen=999)
install.packages("ggrepel")
install.packages('ggthemes')
install_github("hrbrmstr/rgeocodio")
install.packages("ggthemes")
knitr::opts_chunk$set(echo = TRUE)
options(scipen=999)
install.packages("ggrepel")
install.packages('ggthemes')
install_github("hrbrmstr/rgeocodio")
knitr::opts_chunk$set(echo = TRUE)
options(scipen=999)
install.packages("ggrepel")
install.packages('ggthemes')
install.packages('rgeocodio')
library(tidyverse)
library(lubridate)
library(janitor)
library(ggthemes)
library(tidycensus)
library(ggplot2)
library(ggrepel)
library(tigris)
library(sf)
knitr::opts_chunk$set(echo = TRUE)
options(scipen=999)
##install.packages("ggrepel")
##install.packages('ggthemes')
##install.packages('rgeocodio')
library(tidyverse)
library(lubridate)
library(janitor)
library(ggthemes)
library(tidycensus)
library(ggplot2)
library(ggrepel)
library(tigris)
library(sf)
library(rgeocodio)
devtools::install_github("hrbrmstr/rgeocodio")
install.packages("htmltools")
install.packages("htmltools")
install.packages("htmltools")
install.packages('devtools')
library(devtools)
library(devtools)
detach("package:htmltools", unload = TRUE)
detach("package:htmltools", unload = TRUE)
detach("package:htmltools")
