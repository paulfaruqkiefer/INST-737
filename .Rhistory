# Selecting the variables of interest (excluding foreclosure_pc_2020)
variables_of_interest <- c("avg_bed", "tract_homevalue_2020", "tract_medage_2020", "tract_medincome_2020",
"tract_medincome_2010", "foreclosure_pc_2010",
"pct_built_2020_later", "pct_built_2010_2019", "pct_built_2000_2009",
"pct_built_1990_1999", "pct_built_1980_1989", "pct_built_1970_1979",
"pct_built_pre_1960", "pct_0_bd", "pct_1_bd", "pct_2_bd", "pct_3_bd",
"pct_4_more_bd", "poverty_2010", "poverty_2020", "nhwhite_2010", "nhwhite_2020",
"mortgaged_2010", "mortgaged_2015", "mortgaged_2020", "ownoccupied_2010",
"ownoccupied_2015", "ownoccupied_2020", "mortgage_change_2010_2015",
"mortgage_change_2015_2020", "mortgage_change_2010_2020", "ownoccupied_change_2010_2015",
"ownoccupied_change_2015_2020", "ownoccupied_change_2010_2020", "poverty_change_2010_2020",
"nhwhite_change_2010_2020", "medincome_change_2010_2015", "medincome_change_2015_2020",
"medincome_change_2010_2020")
# Create a new dataframe with variables of interest and foreclosure_pc_2020
data_subset <- pg_foreclosures_per_tract[c("foreclosure_pc_2020", variables_of_interest)]
# Create scatterplot matrix
pairs(data_subset)
# Select a subset of variables of interest (excluding foreclosure_pc_2020)
variables_of_interest_subset <- c("avg_bed", "tract_homevalue_2020", "tract_medage_2020", "tract_medincome_2020",
"tract_medincome_2010", "foreclosure_pc_2010")
# Create a new dataframe with variables of interest and foreclosure_pc_2020
data_subset <- pg_foreclosures_per_tract[c("foreclosure_pc_2020", variables_of_interest_subset)]
# Create scatterplot matrix
pairs(data_subset)
# Selecting the variables of interest (excluding foreclosure_pc_2020)
variables_of_interest <- c("avg_bed", "tract_homevalue_2020", "tract_medage_2020", "tract_medincome_2020",
"tract_medincome_2010", "foreclosure_pc_2010",
"pct_built_2020_later", "pct_built_2010_2019", "pct_built_2000_2009",
"pct_built_1990_1999", "pct_built_1980_1989", "pct_built_1970_1979",
"pct_built_pre_1960", "pct_0_bd", "pct_1_bd", "pct_2_bd", "pct_3_bd",
"pct_4_more_bd", "poverty_2010", "poverty_2020", "nhwhite_2010", "nhwhite_2020",
"mortgaged_2010", "mortgaged_2015", "mortgaged_2020", "ownoccupied_2010",
"ownoccupied_2015", "ownoccupied_2020", "mortgage_change_2010_2015",
"mortgage_change_2015_2020", "mortgage_change_2010_2020", "ownoccupied_change_2010_2015",
"ownoccupied_change_2015_2020", "ownoccupied_change_2010_2020", "poverty_change_2010_2020",
"nhwhite_change_2010_2020", "medincome_change_2010_2015", "medincome_change_2015_2020",
"medincome_change_2010_2020")
# Calculate the correlation matrix
correlation_matrix <- cor(pg_foreclosures_per_tract[variables_of_interest], pg_foreclosures_per_tract$foreclosure_pc_2020)
# Print the correlation coefficients
print(correlation_matrix)
library(gplots)
install.packages('gplots')
library(gplots)
# Calculate the correlation matrix
correlation_matrix <- cor(pg_foreclosures_per_tract[variables_of_interest], pg_foreclosures_per_tract$foreclosure_pc_2020)
# Create a heatmap
heatmap.2(correlation_matrix,
trace = "none",        # no borders around heatmap
col = colorRampPalette(c("blue", "white", "red"))(100),  # color palette
margins = c(12, 9),    # more space for row and column labels
main = "Correlation Heatmap")  # title
# Calculate the correlation matrix
correlation_matrix <- cor(pg_foreclosures_per_tract[variables_of_interest], pg_foreclosures_per_tract$foreclosure_pc_2020)
# Create a heatmap
heatmap.2(correlation_matrix,
trace = "none",        # no borders around heatmap
col = colorRampPalette(c("blue", "white", "red"))(100),  # color palette
margins = c(12, 9),    # more space for row and column labels
main = "Correlation Heatmap")  # title
# Calculate the correlation matrix
correlation_matrix <- cor(pg_foreclosures_per_tract[variables_of_interest], pg_foreclosures_per_tract$foreclosure_pc_2020)
# Transpose the correlation matrix
correlation_matrix <- t(correlation_matrix)
# Create a heatmap
heatmap.2(correlation_matrix,
trace = "none",        # no borders around heatmap
col = colorRampPalette(c("blue", "white", "red"))(100),  # color palette
margins = c(12, 9),    # more space for row and column labels
main = "Correlation Heatmap")  # title
# Calculate the correlation coefficients
correlation_coefficients <- cor(pg_foreclosures_per_tract[variables_of_interest], pg_foreclosures_per_tract$foreclosure_pc_2020)
# Convert the correlation coefficients to a data frame
correlation_df <- as.data.frame(correlation_coefficients)
# Exclude the diagonal (correlation of a variable with itself)
correlation_df <- correlation_df[upper.tri(correlation_df)]
# Plot the correlations
plot(correlation_df,
main = "Correlation of Variables with foreclosure_pc_2020",
xlab = "Variable",
ylab = "Correlation Coefficient")
# Calculate the correlation coefficients
correlation_coefficients <- cor(pg_foreclosures_per_tract[variables_of_interest], pg_foreclosures_per_tract$foreclosure_pc_2020)
# Convert the correlation coefficients to a data frame
correlation_df <- as.data.frame(correlation_coefficients)
# Exclude the diagonal (correlation of a variable with itself)
correlation_df <- correlation_df[upper.tri(correlation_df)]
# Plot the correlations
plot(correlation_df,
ylim = c(-1, 1), # Set y-axis limits
main = "Correlation of Variables with foreclosure_pc_2020",
xlab = "Variable",
ylab = "Correlation Coefficient")
knitr::opts_chunk$set(echo = TRUE)
options(scipen=999)
##install.packages("ggrepel")
##install.packages('ggthemes')
##install.packages('gplots')
library(tidyverse)
library(dplyr)
#Necessary for date parsing.
library(lubridate)
#Necessary for column name cleaning.
library(janitor)
#Eventually necessary for visualization
library(ggthemes)
#Necessary for Census API calls.
library(tidycensus)
#Eventually necessary for visualizations.
library(ggplot2)
library(gplots)
#Potentially necessary for cleaning.
library(ggrepel)
#Eventually necessary for visualizations.
library(tigris)
#Simple features.
library(sf)
#Eventually necessary for mapping.
library(ggmap)
# Read in the initial dataset, setting a guess_max limit to improve the accuracy of the data types assigned to each column.
foreclosure_pg <- read_csv("datasets/pg_foreclosures.csv", guess_max = 71676)
#Read in the geocoded address dataset.
foreclosure_pg_census_tracts <- read_csv("datasets/pg_foreclosure_tract_geocodio.csv", guess_max = 71676) |>
clean_names()
# Calculate the number of unique tracts in the geocoded address dataframe.
tracts_unique <- length(unique(foreclosure_pg_census_tracts$census_tract_code))
print(tracts_unique)
# Perform inner join based on the 'location' column.
pg_foreclosures_with_tracts <- inner_join(foreclosure_pg, foreclosure_pg_census_tracts, by = "location")
# Convert "submitteddate" column to date type.
pg_foreclosures_with_tracts$submitteddate <- as.Date(pg_foreclosures_with_tracts$submitteddate, format = "%m/%d/%Y")
# Extract the year and month from "submitteddate".
pg_foreclosures_with_tracts$year <- year(pg_foreclosures_with_tracts$submitteddate)
pg_foreclosures_with_tracts$month <- month(pg_foreclosures_with_tracts$submitteddate)
# Create a year-month column for eventual plotting.
pg_foreclosures_with_tracts$year_month <- as.Date(paste(pg_foreclosures_with_tracts$year, pg_foreclosures_with_tracts$month, "01", sep = "-"), format = "%Y-%m-%d")
# Print the dataset.
print(pg_foreclosures_with_tracts)
# Order the dataframe by the columns that identify remainins duplicate rows.
pg_foreclosures_with_tracts <- pg_foreclosures_with_tracts[order(pg_foreclosures_with_tracts$propertyid, pg_foreclosures_with_tracts$street_address, pg_foreclosures_with_tracts$city.y, pg_foreclosures_with_tracts$submitteddate), ]
# Keep only the last row from each set of duplicates
pg_foreclosures_filtered <- subset(pg_foreclosures_with_tracts, !duplicated(pg_foreclosures_with_tracts[, c("propertyid", "street_address", "city.y", "submitteddate")], fromLast = TRUE))
# Reset row names if necessary
rownames(pg_foreclosures_filtered) <- NULL
# Filter the pg_foreclosures_filtered dataset to remove rows with missing values in the census_tract_code column.
pg_foreclosures_filtered <- pg_foreclosures_filtered[!is.na(pg_foreclosures_filtered$census_tract_code), ]
# Remove unnecessary columns with the select() function.
pg_foreclosures_filtered <- pg_foreclosures_filtered %>%
select(-accuracy_score, -accuracy_type, -source, -full_fips_block, -metro_micro_statistical_area_name,-metro_micro_statistical_area_code, -metro_micro_statistical_area_type, -combined_statistical_area_name, -metropolitan_division_area_name, -metropolitan_division_area_code, -addressoccupied)
knitr::opts_chunk$set(echo = TRUE)
options(scipen=999)
##install.packages("ggrepel")
##install.packages('ggthemes')
##install.packages('gplots')
library(tidyverse)
library(dplyr)
#Necessary for date parsing.
library(lubridate)
#Necessary for column name cleaning.
library(janitor)
#Eventually necessary for visualization
library(ggthemes)
#Necessary for Census API calls.
library(tidycensus)
#Eventually necessary for visualizations.
library(ggplot2)
library(gplots)
#Potentially necessary for cleaning.
library(ggrepel)
#Eventually necessary for visualizations.
library(tigris)
#Simple features.
library(sf)
#Eventually necessary for mapping.
library(ggmap)
# Read in the initial dataset, setting a guess_max limit to improve the accuracy of the data types assigned to each column.
foreclosure_pg <- read_csv("datasets/pg_foreclosures.csv", guess_max = 71676)
#Read in the geocoded address dataset.
foreclosure_pg_census_tracts <- read_csv("datasets/pg_foreclosure_tract_geocodio.csv", guess_max = 71676) |>
clean_names()
# Calculate the number of unique tracts in the geocoded address dataframe.
tracts_unique <- length(unique(foreclosure_pg_census_tracts$census_tract_code))
print(tracts_unique)
# Perform inner join based on the 'location' column.
pg_foreclosures_with_tracts <- inner_join(foreclosure_pg, foreclosure_pg_census_tracts, by = "location")
# Convert "submitteddate" column to date type.
pg_foreclosures_with_tracts$submitteddate <- as.Date(pg_foreclosures_with_tracts$submitteddate, format = "%m/%d/%Y")
# Extract the year and month from "submitteddate".
pg_foreclosures_with_tracts$year <- year(pg_foreclosures_with_tracts$submitteddate)
pg_foreclosures_with_tracts$month <- month(pg_foreclosures_with_tracts$submitteddate)
# Create a year-month column for eventual plotting.
pg_foreclosures_with_tracts$year_month <- as.Date(paste(pg_foreclosures_with_tracts$year, pg_foreclosures_with_tracts$month, "01", sep = "-"), format = "%Y-%m-%d")
# Print the dataset.
print(pg_foreclosures_with_tracts)
# Order the dataframe by the columns that identify remainins duplicate rows.
pg_foreclosures_with_tracts <- pg_foreclosures_with_tracts[order(pg_foreclosures_with_tracts$propertyid, pg_foreclosures_with_tracts$street_address, pg_foreclosures_with_tracts$city.y, pg_foreclosures_with_tracts$submitteddate), ]
# Keep only the last row from each set of duplicates
pg_foreclosures_filtered <- subset(pg_foreclosures_with_tracts, !duplicated(pg_foreclosures_with_tracts[, c("propertyid", "street_address", "city.y", "submitteddate")], fromLast = TRUE))
# Reset row names if necessary
rownames(pg_foreclosures_filtered) <- NULL
# Filter the pg_foreclosures_filtered dataset to remove rows with missing values in the census_tract_code column.
pg_foreclosures_filtered <- pg_foreclosures_filtered[!is.na(pg_foreclosures_filtered$census_tract_code), ]
# Remove unnecessary columns with the select() function.
pg_foreclosures_filtered <- pg_foreclosures_filtered %>%
select(-accuracy_score, -accuracy_type, -source, -full_fips_block, -metro_micro_statistical_area_name,-metro_micro_statistical_area_code, -metro_micro_statistical_area_type, -combined_statistical_area_name, -metropolitan_division_area_name, -metropolitan_division_area_code, -addressoccupied)
knitr::opts_chunk$set(echo = TRUE)
# Read in the initial dataset, setting a guess_max limit to improve the accuracy of the data types assigned to each column.
pg_foreclosures_per_tract <- read_csv("datasets/pg_foreclosures_per_tract.csv", guess_max = 173)
knitr::opts_chunk$set(echo = TRUE)
options(scipen=999)
##install.packages("ggrepel")
##install.packages('ggthemes')
##install.packages('aod')
##install.packages('caret')
##install.packages("MASS")
##install.packages("mice")
##install.packages("pdp")
library(tidyverse)
#Necessary for date parsing.
library(lubridate)
#Necessary for column name cleaning.
library(janitor)
#Eventually necessary for visualization
library(ggthemes)
#Necessary for Census API calls.
library(tidycensus)
#Eventually necessary for visualizations.
library(ggplot2)
#Potentially necessary for cleaning.
library(ggrepel)
#Eventually necessary for visualizations.
library(tigris)
#Simple features.
library(sf)
#Eventually necessary for mapping.
library(ggmap)
#Needed for logistic regression
library(aod)
#Needed for splitting into training and test datasets
library(caret)
#Needed for ordinal logistic regression
library(MASS)
#Needed for imputing missing values (using predictive mean matching)
library(mice)
#Needed for visualizing a partial dependence plot
library(pdp)
#Needed for naive bayes classification
library(e1071)
# Read in the initial dataset, setting a guess_max limit to improve the accuracy of the data types assigned to each column.
pg_foreclosures_per_tract <- read_csv("datasets/pg_foreclosures_per_tract.csv", guess_max = 173)
pg_hmda_loan_data <- read_csv("datasets/pg_tract_hmda.csv", guess_max = 183)
knitr::opts_chunk$set(echo = TRUE)
pg_hmda_loan_data$census_tract_number <- gsub("\\.", "", pg_hmda_loan_data$census_tract_number)
pg_hmda_loan_data <- pg_hmda_loan_data %>%
mutate(tract_number = as.numeric(census_tract_number, na.rm = TRUE)) |>
select(-census_tract_number)
knitr::opts_chunk$set(echo = TRUE)
pg_hmda_loan_data$census_tract_number <- gsub("\\.", "", pg_hmda_loan_data$census_tract_number)
pg_hmda_loan_data <- pg_hmda_loan_data %>%
mutate(tract_number = as.numeric(census_tract_number, na.rm = TRUE))
# Perform a left join based on the tract_number column.
pg_foreclosures_per_tract <- left_join(pg_foreclosures_per_tract, pg_hmda_loan_data, by = c("tract_number" = "census_tract_number"))
knitr::opts_chunk$set(echo = TRUE)
# Perform a left join based on the tract_number column.
pg_foreclosures_per_tract <- left_join(pg_foreclosures_per_tract, pg_hmda_loan_data, by = c("tract_number" = "tract_number"))
View(pg_foreclosures_per_tract)
View(pg_hmda_loan_data)
knitr::opts_chunk$set(echo = TRUE)
# Read in the initial dataset, setting a guess_max limit to improve the accuracy of the data types assigned to each column.
pg_foreclosures_per_tract <- read_csv("datasets/pg_foreclosures_per_tract.csv", guess_max = 173)
# Create a new column "foreclosure_quantile" based on quantiles of foreclosure_pc_2020
pg_foreclosures_per_tract_log_reg <- pg_foreclosures_per_tract %>%
mutate(foreclosure_quantile = ntile(foreclosure_pc_2020, 5),
high_or_low_foreclosure = case_when(
foreclosure_quantile %in% c(1, 2, 3) ~ 0,
foreclosure_quantile %in% c(4, 5) ~ 1,
))
# Set the seed for reproducibility
set.seed(123)
# Split the data into 70% training and 30% test sets
train_index <- createDataPartition(pg_foreclosures_per_tract_log_reg$foreclosure_quantile, p = 0.7, list = FALSE)
# Create training and test sets
train_data <- pg_foreclosures_per_tract_log_reg[train_index, ]
test_data <- pg_foreclosures_per_tract_log_reg[-train_index, ]
# Check the dimensions of training and test sets to ensure they have the appropriate number of columns
dim(train_data)
dim(test_data)
# Check data range for numeric variables
numeric_vars <- train_data[sapply(train_data, is.numeric)]
data_range <- sapply(numeric_vars, function(x) c(min = min(x, na.rm = TRUE), max = max(x, na.rm = TRUE)))
print("Data Range:")
print(data_range)
# Check for infinite values in numeric variables
infinite_values <- colSums(sapply(numeric_vars, is.infinite))
print("Infinite Values:")
print(infinite_values)
# Check for collinear variables (using correlation matrix)
correlation_matrix <- cor(numeric_vars)
highly_correlated <- findCorrelation(correlation_matrix, cutoff = 0.7)
print("Highly Correlated Variables:")
print(colnames(numeric_vars)[highly_correlated])
# Create scatterplot matrix
pairs(train_data[, c("avg_bed", "tract_homevalue_2020", "tract_medage_2020", "tract_medincome_2020",
"tract_medincome_2010", "foreclosure_pc_2010",
"pct_built_2020_later")])
# Convert foreclosure_quantile to factor to make the thing run (not really sure why I need to specify this, but it's the only thing that worked)
train_data$foreclosure_quantile <- factor(train_data$foreclosure_quantile)
# Use the training dataset for model fitting
model_1 <- polr(foreclosure_quantile ~ foreclosure_pc_2010 + tract_medage_2020 + poverty_2010, data = train_data, Hess = TRUE)
# Summarize the model
summary(model_1)
# Convert foreclosure_quantile to factor
train_data$foreclosure_quantile <- factor(train_data$foreclosure_quantile)
# Use the training dataset for model fitting
model_2 <- polr(foreclosure_quantile ~ foreclosure_pc_2010 + tract_medage_2020 + poverty_2010 + nhwhite_2020 + medincome_change_2010_2020, data = train_data, Hess = TRUE)
# Summarize the model
summary(model_2)
# Use the training dataset for model fitting
model_3 <- polr(foreclosure_quantile ~ avg_bed + mortgaged_2010 + pct_1_bd, data = train_data, Hess = TRUE)
# Summarize the model
summary(model_3)
# Use the training dataset for model fitting
model_4 <- polr(foreclosure_quantile ~ avg_bed + mortgaged_2010 + pct_1_bd + pct_built_pre_1960 + pct_built_2000_2009 + mortgage_change_2010_2015, data = train_data, Hess = TRUE)
# Summarize the model
summary(model_4)
# Use the training dataset for model fitting
model_5 <- polr(foreclosure_quantile ~ foreclosure_pc_2010 + tract_medage_2020 + poverty_2010 + nhwhite_2020 + medincome_change_2010_2020 + avg_bed + mortgaged_2010 + pct_1_bd + pct_built_pre_1960 + pct_built_2000_2009 + mortgage_change_2010_2015 + pop_change_pct, data = train_data, Hess = TRUE)
# Summarize the model
summary(model_5)
# Define coefficients
coefficients <- coef(model_5)
# Calculate log-odds (which are just the coefficients)
log_odds <- coefficients
# Calculate odds ratios
odds_ratios <- exp(coefficients)
# Display the results
result <- data.frame(
Predictor_Variable = names(coefficients),
Log_Odds = log_odds,
Odds_Ratio = odds_ratios
)
print(result)
# Predict on test data
predictions <- predict(model_5, newdata = test_data, type = "class")
# View the predictions
print(predictions)
# Extract the actual values from test_data
actual_values <- test_data$foreclosure_quantile
# Compare predicted values with actual values
comparison <- data.frame(Actual = actual_values, Predicted = predictions)
# Count the number of matching rows
matching_rows <- sum(comparison$Actual == comparison$Predicted)
# Calculate the total number of rows
total_rows <- nrow(comparison)
# Calculate the percentage of matching rows
matching_percentage <- (matching_rows / total_rows) * 100
# Print the percentage
cat("Percentage of matching rows:", matching_percentage, "%\n")
pg_foreclosures_per_tract_naive_bayes <- pg_foreclosures_per_tract_log_reg %>%
mutate(
nhwhite_2020_quantile = ntile(nhwhite_2020, 5),
foreclosure_pc_2010_quantile = ntile(foreclosure_pc_2010, 5),
tract_medage_2020_quantile = ntile(tract_medage_2020, 5),
poverty_2010_quantile = ntile(poverty_2010, 5),
medincome_change_2015_2020_quantile = ntile(medincome_change_2015_2020, 5),
avg_bed_quantile = ntile(avg_bed, 5),
mortgaged_2010_quantile = ntile(mortgaged_2010, 5),
pct_1_bd_quantile = ntile(pct_1_bd, 5),
pct_built_pre_1960_quantile = ntile(pct_built_pre_1960, 5),
pct_built_2000_2009_quantile = ntile(pct_built_2000_2009, 5),
mortgage_change_2010_2015_quantile = ntile(mortgage_change_2010_2015, 5)
)
set.seed(123) # For reproducibility
test_percent <- 0.2
indices <- sample(1:nrow(pg_foreclosures_per_tract_naive_bayes),
size = round(test_percent * nrow(pg_foreclosures_per_tract_naive_bayes)))
train_data_2 <- pg_foreclosures_per_tract_naive_bayes[-indices, ]
test_data_2 <- pg_foreclosures_per_tract_naive_bayes[indices, ]
#Train Naive Bayes Model
# Fit the Naive Bayes model
naive_bayes_model <- naiveBayes(foreclosure_quantile ~ nhwhite_2020 +
foreclosure_pc_2010 +
tract_medage_2020 +
poverty_2010 +
medincome_change_2010_2020 +
avg_bed +
mortgaged_2010 +
pct_1_bd +
pct_built_pre_1960 +
pct_built_2000_2009 +
mortgage_change_2010_2015 +
pop_change_pct, data = pg_foreclosures_per_tract_naive_bayes)
# Summary of the model
summary(naive_bayes_model)
# Predict using the Naive Bayes model
predictions <- predict(naive_bayes_model, newdata = test_data_2)
# Create the confusion matrix
confusion_matrix <- confusionMatrix(predictions, factor(test_data_2$foreclosure_quantile, levels = 1:5))
# Print the confusion matrix
print(confusion_matrix)
knitr::opts_chunk$set(echo = TRUE)
options(scipen=999)
##install.packages("ggrepel")
##install.packages('ggthemes')
##install.packages('gplots')
library(tidyverse)
library(dplyr)
#Necessary for date parsing.
library(lubridate)
#Necessary for column name cleaning.
library(janitor)
#Eventually necessary for visualization
library(ggthemes)
#Necessary for Census API calls.
library(tidycensus)
#Eventually necessary for visualizations.
library(ggplot2)
library(gplots)
#Potentially necessary for cleaning.
library(ggrepel)
#Eventually necessary for visualizations.
library(tigris)
#Simple features.
library(sf)
#Eventually necessary for mapping.
library(ggmap)
# Read in the initial dataset, setting a guess_max limit to improve the accuracy of the data types assigned to each column.
foreclosure_pg <- read_csv("datasets/pg_foreclosures.csv", guess_max = 71676)
#Read in the geocoded address dataset.
foreclosure_pg_census_tracts <- read_csv("datasets/pg_foreclosure_tract_geocodio.csv", guess_max = 71676) |>
clean_names()
# Calculate the number of unique tracts in the geocoded address dataframe.
tracts_unique <- length(unique(foreclosure_pg_census_tracts$census_tract_code))
print(tracts_unique)
# Perform inner join based on the 'location' column.
pg_foreclosures_with_tracts <- inner_join(foreclosure_pg, foreclosure_pg_census_tracts, by = "location")
# Convert "submitteddate" column to date type.
pg_foreclosures_with_tracts$submitteddate <- as.Date(pg_foreclosures_with_tracts$submitteddate, format = "%m/%d/%Y")
# Extract the year and month from "submitteddate".
pg_foreclosures_with_tracts$year <- year(pg_foreclosures_with_tracts$submitteddate)
pg_foreclosures_with_tracts$month <- month(pg_foreclosures_with_tracts$submitteddate)
# Create a year-month column for eventual plotting.
pg_foreclosures_with_tracts$year_month <- as.Date(paste(pg_foreclosures_with_tracts$year, pg_foreclosures_with_tracts$month, "01", sep = "-"), format = "%Y-%m-%d")
# Print the dataset.
print(pg_foreclosures_with_tracts)
# Order the dataframe by the columns that identify remainins duplicate rows.
pg_foreclosures_with_tracts <- pg_foreclosures_with_tracts[order(pg_foreclosures_with_tracts$propertyid, pg_foreclosures_with_tracts$street_address, pg_foreclosures_with_tracts$city.y, pg_foreclosures_with_tracts$submitteddate), ]
# Keep only the last row from each set of duplicates
pg_foreclosures_filtered <- subset(pg_foreclosures_with_tracts, !duplicated(pg_foreclosures_with_tracts[, c("propertyid", "street_address", "city.y", "submitteddate")], fromLast = TRUE))
# Reset row names if necessary
rownames(pg_foreclosures_filtered) <- NULL
# Filter the pg_foreclosures_filtered dataset to remove rows with missing values in the census_tract_code column.
pg_foreclosures_filtered <- pg_foreclosures_filtered[!is.na(pg_foreclosures_filtered$census_tract_code), ]
# Remove unnecessary columns with the select() function.
pg_foreclosures_filtered <- pg_foreclosures_filtered %>%
select(-accuracy_score, -accuracy_type, -source, -full_fips_block, -metro_micro_statistical_area_name,-metro_micro_statistical_area_code, -metro_micro_statistical_area_type, -combined_statistical_area_name, -metropolitan_division_area_name, -metropolitan_division_area_code, -addressoccupied)
knitr::opts_chunk$set(echo = TRUE)
options(scipen=999)
##install.packages("ggrepel")
##install.packages('ggthemes')
##install.packages('gplots')
library(tidyverse)
library(dplyr)
#Necessary for date parsing.
library(lubridate)
#Necessary for column name cleaning.
library(janitor)
#Eventually necessary for visualization
library(ggthemes)
#Necessary for Census API calls.
library(tidycensus)
#Eventually necessary for visualizations.
library(ggplot2)
library(gplots)
#Potentially necessary for cleaning.
library(ggrepel)
#Eventually necessary for visualizations.
library(tigris)
#Simple features.
library(sf)
#Eventually necessary for mapping.
library(ggmap)
# Read in the initial dataset, setting a guess_max limit to improve the accuracy of the data types assigned to each column.
foreclosure_pg <- read_csv("datasets/pg_foreclosures.csv", guess_max = 71676)
#Read in the geocoded address dataset.
foreclosure_pg_census_tracts <- read_csv("datasets/pg_foreclosure_tract_geocodio.csv", guess_max = 71676) |>
clean_names()
# Calculate the number of unique tracts in the geocoded address dataframe.
tracts_unique <- length(unique(foreclosure_pg_census_tracts$census_tract_code))
print(tracts_unique)
# Perform inner join based on the 'location' column.
pg_foreclosures_with_tracts <- inner_join(foreclosure_pg, foreclosure_pg_census_tracts, by = "location")
# Convert "submitteddate" column to date type.
pg_foreclosures_with_tracts$submitteddate <- as.Date(pg_foreclosures_with_tracts$submitteddate, format = "%m/%d/%Y")
# Extract the year and month from "submitteddate".
pg_foreclosures_with_tracts$year <- year(pg_foreclosures_with_tracts$submitteddate)
pg_foreclosures_with_tracts$month <- month(pg_foreclosures_with_tracts$submitteddate)
# Create a year-month column for eventual plotting.
pg_foreclosures_with_tracts$year_month <- as.Date(paste(pg_foreclosures_with_tracts$year, pg_foreclosures_with_tracts$month, "01", sep = "-"), format = "%Y-%m-%d")
# Print the dataset.
print(pg_foreclosures_with_tracts)
# Order the dataframe by the columns that identify remainins duplicate rows.
pg_foreclosures_with_tracts <- pg_foreclosures_with_tracts[order(pg_foreclosures_with_tracts$propertyid, pg_foreclosures_with_tracts$street_address, pg_foreclosures_with_tracts$city.y, pg_foreclosures_with_tracts$submitteddate), ]
# Keep only the last row from each set of duplicates
pg_foreclosures_filtered <- subset(pg_foreclosures_with_tracts, !duplicated(pg_foreclosures_with_tracts[, c("propertyid", "street_address", "city.y", "submitteddate")], fromLast = TRUE))
# Reset row names if necessary
rownames(pg_foreclosures_filtered) <- NULL
# Filter the pg_foreclosures_filtered dataset to remove rows with missing values in the census_tract_code column.
pg_foreclosures_filtered <- pg_foreclosures_filtered[!is.na(pg_foreclosures_filtered$census_tract_code), ]
# Remove unnecessary columns with the select() function.
pg_foreclosures_filtered <- pg_foreclosures_filtered %>%
select(-accuracy_score, -accuracy_type, -source, -full_fips_block, -metro_micro_statistical_area_name,-metro_micro_statistical_area_code, -metro_micro_statistical_area_type, -combined_statistical_area_name, -metropolitan_division_area_name, -metropolitan_division_area_code, -addressoccupied)
