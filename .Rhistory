# Plot using ggplot2
ggplot(melted_data, aes(x = foreclosure_pc_2020, y = value, color = variable)) +
geom_line() +
facet_wrap(~foreclosure_quantile, scales = "free_y") +
labs(x = "Foreclosure Percentage 2020", y = "Probability", color = "Variable") +
theme_minimal()
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
options(scipen=999)
##install.packages("ggrepel")
##install.packages('ggthemes')
##install.packages('aod')
##install.packages('caret')
##install.packages("MASS")
##install.packages("mice")
##install.packages("pdp")
library(tidyverse)
#Necessary for date parsing.
library(lubridate)
#Necessary for column name cleaning.
library(janitor)
#Eventually necessary for visualization
library(ggthemes)
#Necessary for Census API calls.
library(tidycensus)
#Eventually necessary for visualizations.
library(ggplot2)
#Potentially necessary for cleaning.
library(ggrepel)
#Eventually necessary for visualizations.
library(tigris)
#Simple features.
library(sf)
#Eventually necessary for mapping.
library(ggmap)
#Needed for logistic regression
library(aod)
#Needed for splitting into training and test datasets
library(caret)
#Needed for ordinal logistic regression
library(MASS)
#Needed for imputing missing values (using predictive mean matching)
library(mice)
#Needed for visualizing a partial dependence plot
library(pdp)
#Needed for naive bayes classification
library(e1071)
knitr::opts_chunk$set(echo = TRUE)
options(scipen=999)
##install.packages("ggrepel")
##install.packages('ggthemes')
##install.packages('aod')
##install.packages('caret')
##install.packages("MASS")
##install.packages("mice")
##install.packages("pdp")
library(tidyverse)
#Necessary for date parsing.
library(lubridate)
#Necessary for column name cleaning.
library(janitor)
#Eventually necessary for visualization
library(ggthemes)
#Necessary for Census API calls.
library(tidycensus)
#Eventually necessary for visualizations.
library(ggplot2)
#Potentially necessary for cleaning.
library(ggrepel)
#Eventually necessary for visualizations.
library(tigris)
#Simple features.
library(sf)
#Eventually necessary for mapping.
library(ggmap)
#Needed for logistic regression
library(aod)
#Needed for splitting into training and test datasets
library(caret)
#Needed for ordinal logistic regression
library(MASS)
#Needed for imputing missing values (using predictive mean matching)
library(mice)
#Needed for visualizing a partial dependence plot
library(pdp)
#Needed for naive bayes classification
library(e1071)
# Read in the initial dataset, setting a guess_max limit to improve the accuracy of the data types assigned to each column.
pg_foreclosures_per_tract <- read_csv("datasets/pg_foreclosures_per_tract.csv", guess_max = 173)
# Create a new column "foreclosure_quantile" based on quantiles of foreclosure_pc_2020
pg_foreclosures_per_tract_log_reg <- pg_foreclosures_per_tract %>%
mutate(foreclosure_quantile = ntile(foreclosure_pc_2020, 5),
high_or_low_foreclosure = case_when(
foreclosure_quantile %in% c(1, 2, 3) ~ 0,
foreclosure_quantile %in% c(4, 5) ~ 1,
))
# Set the seed for reproducibility
set.seed(123)
# Split the data into 70% training and 30% test sets
train_index <- createDataPartition(pg_foreclosures_per_tract_log_reg$foreclosure_quantile, p = 0.7, list = FALSE)
# Create training and test sets
train_data <- pg_foreclosures_per_tract_log_reg[train_index, ]
test_data <- pg_foreclosures_per_tract_log_reg[-train_index, ]
# Check the dimensions of training and test sets to ensure they have the appropriate number of columns
dim(train_data)
dim(test_data)
# Check data range for numeric variables
numeric_vars <- train_data[sapply(train_data, is.numeric)]
data_range <- sapply(numeric_vars, function(x) c(min = min(x, na.rm = TRUE), max = max(x, na.rm = TRUE)))
print("Data Range:")
print(data_range)
# Check for infinite values in numeric variables
infinite_values <- colSums(sapply(numeric_vars, is.infinite))
print("Infinite Values:")
print(infinite_values)
# Check for collinear variables (using correlation matrix)
correlation_matrix <- cor(numeric_vars)
highly_correlated <- findCorrelation(correlation_matrix, cutoff = 0.8)
print("Highly Correlated Variables:")
print(colnames(numeric_vars)[highly_correlated])
# Create scatterplot matrix
pairs(train_data[, c("avg_bed", "tract_homevalue_2020", "tract_medage_2020", "tract_medincome_2020",
"tract_medincome_2010", "foreclosure_pc_2010",
"pct_built_2020_later")])
# Convert foreclosure_quantile to factor to make the thing run (not really sure why I need to specify this, but it's the only thing that worked)
train_data$foreclosure_quantile <- factor(train_data$foreclosure_quantile)
# Use the training dataset for model fitting
model_1 <- polr(foreclosure_quantile ~ foreclosure_pc_2010 + tract_medage_2020 + poverty_2010, data = train_data, Hess = TRUE)
# Summarize the model
summary(model_1)
# Predict probabilities for each level of foreclosure_quantile
predicted_probabilities <- predict(model_1, type = "probs")
# Combine predicted probabilities with original data
predicted_data <- cbind(train_data, predicted_probabilities)
# Melt the data for easy plotting
melted_data <- reshape2::melt(predicted_data, id.vars = c("foreclosure_quantile"))
# Plot using ggplot2
ggplot(melted_data, aes(x = foreclosure_pc_2020, y = value, color = variable)) +
geom_line() +
facet_wrap(~foreclosure_quantile, scales = "free_y") +
labs(x = "Foreclosure Percentage 2020", y = "Probability", color = "Variable") +
theme_minimal()
knitr::opts_chunk$set(echo = TRUE)
# Predict probabilities for each level of foreclosure_quantile
predicted_probabilities <- predict(model_1, type = "probs")
# Combine predicted probabilities with original data
predicted_data <- cbind(train_data, predicted_probabilities)
# Melt the data for easy plotting
melted_data <- reshape2::melt(predicted_data, id.vars = c("foreclosure_quantile"))
# Plot using ggplot2
ggplot(melted_data, aes(x = train_data$foreclosure_pc_2020, y = value, color = variable)) +
geom_line() +
facet_wrap(~foreclosure_quantile, scales = "free_y") +
labs(x = "Foreclosure Percentage 2020", y = "Probability", color = "Variable") +
theme_minimal()
View(train_data)
knitr::opts_chunk$set(echo = TRUE)
# Predict probabilities for each level of foreclosure_quantile
predicted_probabilities <- predict(model_1, type = "probs")
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
options(scipen=999)
##install.packages("ggrepel")
##install.packages('ggthemes')
##install.packages('aod')
##install.packages('caret')
##install.packages("MASS")
##install.packages("mice")
##install.packages("pdp")
library(tidyverse)
#Necessary for date parsing.
library(lubridate)
#Necessary for column name cleaning.
library(janitor)
#Eventually necessary for visualization
library(ggthemes)
#Necessary for Census API calls.
library(tidycensus)
#Eventually necessary for visualizations.
library(ggplot2)
#Potentially necessary for cleaning.
library(ggrepel)
#Eventually necessary for visualizations.
library(tigris)
#Simple features.
library(sf)
#Eventually necessary for mapping.
library(ggmap)
#Needed for logistic regression
library(aod)
#Needed for splitting into training and test datasets
library(caret)
#Needed for ordinal logistic regression
library(MASS)
#Needed for imputing missing values (using predictive mean matching)
library(mice)
#Needed for visualizing a partial dependence plot
library(pdp)
#Needed for naive bayes classification
library(e1071)
# Read in the initial dataset, setting a guess_max limit to improve the accuracy of the data types assigned to each column.
pg_foreclosures_per_tract <- read_csv("datasets/pg_foreclosures_per_tract.csv", guess_max = 173)
# Create a new column "foreclosure_quantile" based on quantiles of foreclosure_pc_2020
pg_foreclosures_per_tract_log_reg <- pg_foreclosures_per_tract %>%
mutate(foreclosure_quantile = ntile(foreclosure_pc_2020, 5),
high_or_low_foreclosure = case_when(
foreclosure_quantile %in% c(1, 2, 3) ~ 0,
foreclosure_quantile %in% c(4, 5) ~ 1,
))
# Set the seed for reproducibility
set.seed(123)
# Split the data into 70% training and 30% test sets
train_index <- createDataPartition(pg_foreclosures_per_tract_log_reg$foreclosure_quantile, p = 0.7, list = FALSE)
# Create training and test sets
train_data <- pg_foreclosures_per_tract_log_reg[train_index, ]
test_data <- pg_foreclosures_per_tract_log_reg[-train_index, ]
# Check the dimensions of training and test sets to ensure they have the appropriate number of columns
dim(train_data)
dim(test_data)
# Check data range for numeric variables
numeric_vars <- train_data[sapply(train_data, is.numeric)]
data_range <- sapply(numeric_vars, function(x) c(min = min(x, na.rm = TRUE), max = max(x, na.rm = TRUE)))
print("Data Range:")
print(data_range)
# Check for infinite values in numeric variables
infinite_values <- colSums(sapply(numeric_vars, is.infinite))
print("Infinite Values:")
print(infinite_values)
# Check for collinear variables (using correlation matrix)
correlation_matrix <- cor(numeric_vars)
highly_correlated <- findCorrelation(correlation_matrix, cutoff = 0.8)
print("Highly Correlated Variables:")
print(colnames(numeric_vars)[highly_correlated])
# Create scatterplot matrix
pairs(train_data[, c("avg_bed", "tract_homevalue_2020", "tract_medage_2020", "tract_medincome_2020",
"tract_medincome_2010", "foreclosure_pc_2010",
"pct_built_2020_later")])
# Convert foreclosure_quantile to factor to make the thing run (not really sure why I need to specify this, but it's the only thing that worked)
train_data$foreclosure_quantile <- factor(train_data$foreclosure_quantile)
# Use the training dataset for model fitting
model_1 <- polr(foreclosure_quantile ~ foreclosure_pc_2010 + tract_medage_2020 + poverty_2010, data = train_data, Hess = TRUE)
# Summarize the model
summary(model_1)
# Predict probabilities for each level of foreclosure_quantile
predicted_probabilities <- predict(model_1, type = "probs")
# Combine predicted probabilities with original data
predicted_data <- cbind(train_data, predicted_probabilities)
# Melt the data for easy plotting
melted_data <- reshape2::melt(predicted_data, id.vars = c("foreclosure_quantile"))
# Plot using ggplot2
ggplot(melted_data, aes(x = foreclosure_pc_2020, y = value, color = variable)) +
geom_line() +
facet_wrap(~foreclosure_quantile, scales = "free_y") +
labs(x = "Foreclosure Percentage 2020", y = "Probability", color = "Variable") +
theme_minimal()
knitr::opts_chunk$set(echo = TRUE)
# Predict probabilities for each level of foreclosure_quantile
train_data$predicted_probabilities <- predict(model_1, type = "probs")
# Plot logistic regression curve
ggplot(train_data, aes(x = foreclosure_pc_2010, y = predicted_probabilities, color = foreclosure_quantile)) +
geom_line() +
labs(x = "Foreclosure Percentage 2010", y = "Predicted Probability", color = "Foreclosure Quantile") +
theme_minimal()
knitr::opts_chunk$set(echo = TRUE)
# Predict probabilities for each level of foreclosure_quantile
predicted_probabilities <- predict(model_1, type = "probs")
# Extract the probabilities for each level of foreclosure_quantile
train_data$predicted_probabilities <- predicted_probabilities[, "1"]
# Plot logistic regression curve
ggplot(train_data, aes(x = foreclosure_pc_2010, y = predicted_probabilities, color = foreclosure_quantile)) +
geom_line() +
labs(x = "Foreclosure Percentage 2010", y = "Predicted Probability", color = "Foreclosure Quantile") +
theme_minimal()
knitr::opts_chunk$set(echo = TRUE)
# Predict probabilities for each level of foreclosure_quantile
predicted_probabilities <- predict(model_1, type = "probs")
# Extract the probabilities for each level of foreclosure_quantile
train_data$predicted_probabilities <- predicted_probabilities[, "1"]
# Plot logistic regression curve
ggplot(train_data, aes(x = foreclosure_quantile, y = predicted_probabilities, color = foreclosure_quantile)) +
geom_line() +
labs(x = "Foreclosure Percentage 2010", y = "Predicted Probability", color = "Foreclosure Quantile") +
theme_minimal()
knitr::opts_chunk$set(echo = TRUE)
# Create a sequence of values for foreclosure_pc_2010
foreclosure_pc_values <- seq(min(train_data$foreclosure_quantile), max(train_data$foreclosure_quantile), length.out = 100)
knitr::opts_chunk$set(echo = TRUE)
# Create a sequence of values for foreclosure_pc_2010
foreclosure_pc_values <- seq(min(train_data$foreclosure_pc_2010), max(train_data$foreclosure_pc_2010), length.out = 100)
# Generate predicted probabilities for each value of foreclosure_pc_2010
predicted_probabilities <- predict(model_1, newdata = data.frame(foreclosure_pc_2010 = foreclosure_pc_values), type = "probs")
knitr::opts_chunk$set(echo = TRUE)
# Create a sequence of values for foreclosure_pc_2010
foreclosure_pc_values <- seq(min(train_data$foreclosure_pc_2010), max(train_data$foreclosure_pc_2010), length.out = 100)
# Create a data frame with all predictor variables (except the response variable)
newdata <- expand.grid(foreclosure_pc_2010 = foreclosure_pc_values,
tract_medage_2020 = median(train_data$tract_medage_2020),
poverty_2010 = median(train_data$poverty_2010))
# Generate predicted probabilities for each value of foreclosure_pc_2010
predicted_probabilities <- predict(model_1, newdata = newdata, type = "probs")
# Convert predicted probabilities to predicted quantiles
predicted_quantiles <- apply(predicted_probabilities, 1, function(x) sum(cumsum(x) <= 0.5))
# Create a data frame for plotting
plot_data <- data.frame(foreclosure_pc_2010 = foreclosure_pc_values,
predicted_quantile = predicted_quantiles)
# Plot logistic regression curve
ggplot(plot_data, aes(x = foreclosure_pc_2010, y = predicted_quantile)) +
geom_line() +
labs(x = "Foreclosure Percentage 2010", y = "Predicted Quantile") +
theme_minimal()
knitr::opts_chunk$set(echo = TRUE)
# Extract coefficients from the model summary
coefficients <- summary(model_1)$coefficients
# Remove intercept (if present)
coefficients <- coefficients[-1, ]
# Calculate absolute values of coefficients
coefficients$abs_coef <- abs(coefficients$Value)
knitr::opts_chunk$set(echo = TRUE)
# Extract coefficients from the model summary
coefficients <- coef(model_1)
# Remove intercept (if present)
coefficients <- coefficients[-1]
# Calculate absolute values of coefficients
abs_coefficients <- abs(coefficients)
# Create a data frame for plotting
coefficients_df <- data.frame(variable = names(abs_coefficients), importance = abs_coefficients)
# Order coefficients by absolute value
coefficients_df <- coefficients_df[order(coefficients_df$importance, decreasing = TRUE), ]
# Plot variable importance
ggplot(coefficients_df, aes(x = reorder(variable, importance), y = importance)) +
geom_bar(stat = "identity", fill = "skyblue") +
labs(x = "Predictor Variable", y = "Absolute Coefficient") +
theme_minimal() +
coord_flip() # for horizontal bars
# Convert foreclosure_quantile to factor to make the thing run (not really sure why I need to specify this, but it's the only thing that worked)
train_data$foreclosure_quantile <- factor(train_data$foreclosure_quantile)
# Use the training dataset for model fitting
model_1 <- polr(foreclosure_quantile ~ foreclosure_pc_2010 + tract_medage_2020 + poverty_2010, data = train_data, Hess = TRUE)
# Summarize the model
summary(model_1)
knitr::opts_chunk$set(echo = TRUE)
# Extract coefficients from the model summary
coefficients <- coef(model_1)
# Remove intercept (if present)
coefficients <- coefficients[-1]
# Calculate absolute values of coefficients
abs_coefficients <- abs(coefficients)
# Create a data frame for plotting
coefficients_df <- data.frame(variable = names(abs_coefficients), importance = abs_coefficients)
# Add foreclosure_pc_2010 if it's missing
if (!"foreclosure_pc_2010" %in% coefficients_df$variable) {
coefficients_df <- rbind(coefficients_df, data.frame(variable = "foreclosure_pc_2010", importance = 0))
}
# Order coefficients by absolute value
coefficients_df <- coefficients_df[order(coefficients_df$importance, decreasing = TRUE), ]
# Plot variable importance
ggplot(coefficients_df, aes(x = reorder(variable, importance), y = importance)) +
geom_bar(stat = "identity", fill = "skyblue") +
labs(x = "Predictor Variable", y = "Absolute Coefficient") +
theme_minimal() +
coord_flip() # for horizontal bars
knitr::opts_chunk$set(echo = TRUE)
train_data$foreclosure_quantile <- factor(train_data$foreclosure_quantile)
# Use the training dataset for model fitting
model_1 <- polr(foreclosure_quantile ~ ownoccupied_2010 + tract_homevalue_2020 + tract_medage_2020 +   tract_medincome_2020 + foreclosure_pc_2010 + pct_built_2000_2009 + poverty_2010 + nhwhite_2020 + mortgage_change_2010_2020 + poverty_change_2010_2020 + nhwhite_change_2010_2020 + medincome_change_2010_2015 + medincome_change_2015_2020 + medincome_change_2010_2020 + pop_change_pct, data = train_data, Hess = TRUE)
# Summarize the model
summary(model_1)
knitr::opts_chunk$set(echo = TRUE)
# Check for constant variables
constant_vars <- sapply(train_data, function(x) length(unique(x))) == 1
constant_vars <- names(constant_vars[constant_vars])
print(constant_vars)
knitr::opts_chunk$set(echo = TRUE)
# Check for collinearity
correlation_matrix <- cor(train_data[, -which(names(train_data) == "foreclosure_quantile")])
high_correlation <- findCorrelation(correlation_matrix, cutoff = 0.7)
print(names(train_data)[-which(names(train_data) == "foreclosure_quantile")][high_correlation])
knitr::opts_chunk$set(echo = TRUE)
# Select the variables we listed
selected_vars <- c("mortgaged_2010", "ownoccupied_2010", "mortgaged_2015", "mortgaged_2020",
"ownoccupied_2015", "ownoccupied_2020", "tract_medincome_2010",
"tract_medincome_2020", "pct_4_more_bd", "predicted_probabilities",
"avg_bed", "post_2010_foreclosures", "foreclosure_pc_2020",
"great_recession_foreclosures", "mortgage_change_2010_2020",
"nhwhite_2020", "ownoccupied_change_2010_2015", "ownoccupied_change_2015_2020",
"tract_pop_2010")
# Subset the data with selected variables
selected_data <- train_data[selected_vars]
# Calculate the correlation matrix
correlation_matrix <- cor(selected_data)
# Find pairs of variables with correlation coefficient > 0.7
high_correlation <- which(upper.tri(correlation_matrix, diag = TRUE) & correlation_matrix > 0.7, arr.ind = TRUE)
# Print the collinear variable pairs
collinear_pairs <- data.frame(variable1 = rownames(correlation_matrix)[high_correlation[, 1]],
variable2 = colnames(correlation_matrix)[high_correlation[, 2]],
correlation = correlation_matrix[high_correlation])
print(collinear_pairs)
knitr::opts_chunk$set(echo = TRUE)
# Select the variables from the regression formula
selected_vars <- c("ownoccupied_2010", "tract_homevalue_2020", "tract_medage_2020",
"tract_medincome_2020", "foreclosure_pc_2010", "pct_built_2000_2009",
"poverty_2010", "nhwhite_2020", "mortgage_change_2010_2020",
"poverty_change_2010_2020", "nhwhite_change_2010_2020",
"medincome_change_2010_2015", "medincome_change_2015_2020",
"medincome_change_2010_2020", "pop_change_pct")
# Subset the data with selected variables
selected_data <- train_data[selected_vars]
# Calculate the correlation matrix
correlation_matrix <- cor(selected_data)
# Find pairs of variables with correlation coefficient > 0.7
high_correlation <- which(upper.tri(correlation_matrix, diag = TRUE) & correlation_matrix > 0.7, arr.ind = TRUE)
# Print the collinear variable pairs
collinear_pairs <- data.frame(variable1 = rownames(correlation_matrix)[high_correlation[, 1]],
variable2 = colnames(correlation_matrix)[high_correlation[, 2]],
correlation = correlation_matrix[high_correlation])
print(collinear_pairs)
knitr::opts_chunk$set(echo = TRUE)
train_data$foreclosure_quantile <- factor(train_data$foreclosure_quantile)
# Use the training dataset for model fitting
model_1 <- polr(foreclosure_quantile ~ ownoccupied_2010 + tract_homevalue_2020 + tract_medage_2020 + foreclosure_pc_2010 + pct_built_2000_2009 + poverty_2010 + nhwhite_2020 + mortgage_change_2010_2020 + poverty_change_2010_2020 + nhwhite_change_2010_2020 + medincome_change_2010_2015 + medincome_change_2015_2020 + medincome_change_2010_2020 + pop_change_pct, data = train_data, Hess = TRUE)
# Summarize the model
summary(model_1)
knitr::opts_chunk$set(echo = TRUE)
train_data$foreclosure_quantile <- factor(train_data$foreclosure_quantile)
# Use the training dataset for model fitting
model_1 <- polr(foreclosure_quantile ~ ownoccupied_2010 + tract_medage_2020  foreclosure_pc_2010 + pct_built_2000_2009 + poverty_2010 + nhwhite_2020 + mortgage_change_2010_2020 + poverty_change_2010_2020 + nhwhite_change_2010_2020 + medincome_change_2010_2015 + medincome_change_2015_2020 + medincome_change_2010_2020 + pop_change_pct, data = train_data, Hess = TRUE)
knitr::opts_chunk$set(echo = TRUE)
train_data$foreclosure_quantile <- factor(train_data$foreclosure_quantile)
# Use the training dataset for model fitting
model_1 <- polr(foreclosure_quantile ~ ownoccupied_2010 + tract_medage_2020 + foreclosure_pc_2010 + pct_built_2000_2009 + poverty_2010 + nhwhite_2020 + mortgage_change_2010_2020 + poverty_change_2010_2020 + nhwhite_change_2010_2020 + medincome_change_2010_2015 + medincome_change_2015_2020 + medincome_change_2010_2020 + pop_change_pct, data = train_data, Hess = TRUE)
# Summarize the model
summary(model_1)
knitr::opts_chunk$set(echo = TRUE)
train_data$foreclosure_quantile <- factor(train_data$foreclosure_quantile)
# Use the training dataset for model fitting
model_1 <- polr(foreclosure_quantile ~ ownoccupied_2010 + tract_homevalue_2020 + tract_medage_2020  foreclosure_pc_2010 + pct_built_2000_2009 + poverty_2010 + nhwhite_2020 + mortgage_change_2010_2020 + poverty_change_2010_2020 + nhwhite_change_2010_2020 + medincome_change_2010_2015 + medincome_change_2015_2020 + medincome_change_2010_2020 + pop_change_pct, data = train_data, Hess = TRUE)
knitr::opts_chunk$set(echo = TRUE)
train_data$foreclosure_quantile <- factor(train_data$foreclosure_quantile)
# Use the training dataset for model fitting
model_1 <- polr(foreclosure_quantile ~ ownoccupied_2010 + tract_homevalue_2020 + tract_medage_2020 +   tract_medincome_2020 + foreclosure_pc_2010 + pct_built_2000_2009 + poverty_2010 + nhwhite_2020 + mortgage_change_2010_2020 + poverty_change_2010_2020 + nhwhite_change_2010_2020 + medincome_change_2010_2015 + medincome_change_2015_2020 + medincome_change_2010_2020 + pop_change_pct, data = train_data, Hess = TRUE)
# Summarize the model
summary(model_1)
knitr::opts_chunk$set(echo = TRUE)
train_data$foreclosure_quantile <- factor(train_data$foreclosure_quantile)
# Use the training dataset for model fitting
model_1 <- polr(foreclosure_quantile ~ ownoccupied_2010 + tract_medage_2020 + foreclosure_pc_2010 + pct_built_2000_2009 + poverty_2010 + nhwhite_2020 + mortgage_change_2010_2020 + poverty_change_2010_2020 + nhwhite_change_2010_2020 + medincome_change_2010_2015 + medincome_change_2015_2020 + medincome_change_2010_2020 + pop_change_pct, data = train_data, Hess = TRUE)
# Summarize the model
summary(model_1)
knitr::opts_chunk$set(echo = TRUE)
options(scipen=999)
##install.packages("ggrepel")
##install.packages('ggthemes')
##install.packages('gplots')
#install.packages('car')
##install.packages("glmnet")
library(tidyverse)
library(dplyr)
#Necessary for date parsing.
library(lubridate)
#Necessary for column name cleaning.
library(janitor)
#Eventually necessary for visualization
library(ggthemes)
#Necessary for Census API calls.
library(tidycensus)
#Eventually necessary for visualizations.
library(ggplot2)
library(gplots)
#Potentially necessary for cleaning.
library(ggrepel)
#Eventually necessary for visualizations.
library(tigris)
#Simple features.
library(sf)
#Eventually necessary for mapping.
library(ggmap)
#Needed for dividing into training and test sets.
library(caret)
#Needed for identifying collinear variables.
library(car)
#Needed for regularization
library(glmnet)
# Read in the initial dataset, setting a guess_max limit to improve the accuracy of the data types assigned to each column.
foreclosure_pg <- read_csv("datasets/pg_foreclosures.csv", guess_max = 71676)
#Read in the geocoded address dataset.
foreclosure_pg_census_tracts <- read_csv("datasets/pg_foreclosure_tract_geocodio.csv", guess_max = 71676) |>
clean_names()
# Calculate the number of unique tracts in the geocoded address dataframe.
tracts_unique <- length(unique(foreclosure_pg_census_tracts$census_tract_code))
print(tracts_unique)
# Perform inner join based on the 'location' column.
pg_foreclosures_with_tracts <- inner_join(foreclosure_pg, foreclosure_pg_census_tracts, by = "location")
# Convert "submitteddate" column to date type.
pg_foreclosures_with_tracts$submitteddate <- as.Date(pg_foreclosures_with_tracts$submitteddate, format = "%m/%d/%Y")
# Extract the year and month from "submitteddate".
pg_foreclosures_with_tracts$year <- year(pg_foreclosures_with_tracts$submitteddate)
pg_foreclosures_with_tracts$month <- month(pg_foreclosures_with_tracts$submitteddate)
# Create a year-month column for eventual plotting.
pg_foreclosures_with_tracts$year_month <- as.Date(paste(pg_foreclosures_with_tracts$year, pg_foreclosures_with_tracts$month, "01", sep = "-"), format = "%Y-%m-%d")
# Print the dataset.
print(pg_foreclosures_with_tracts)
# Order the dataframe by the columns that identify remainins duplicate rows.
pg_foreclosures_with_tracts <- pg_foreclosures_with_tracts[order(pg_foreclosures_with_tracts$propertyid, pg_foreclosures_with_tracts$street_address, pg_foreclosures_with_tracts$city.y, pg_foreclosures_with_tracts$submitteddate), ]
# Keep only the last row from each set of duplicates
pg_foreclosures_filtered <- subset(pg_foreclosures_with_tracts, !duplicated(pg_foreclosures_with_tracts[, c("propertyid", "street_address", "city.y", "submitteddate")], fromLast = TRUE))
# Reset row names if necessary
rownames(pg_foreclosures_filtered) <- NULL
# Filter the pg_foreclosures_filtered dataset to remove rows with missing values in the census_tract_code column.
pg_foreclosures_filtered <- pg_foreclosures_filtered[!is.na(pg_foreclosures_filtered$census_tract_code), ]
# Remove unnecessary columns with the select() function.
pg_foreclosures_filtered <- pg_foreclosures_filtered %>%
select(-accuracy_score, -accuracy_type, -source, -full_fips_block, -metro_micro_statistical_area_name,-metro_micro_statistical_area_code, -metro_micro_statistical_area_type, -combined_statistical_area_name, -metropolitan_division_area_name, -metropolitan_division_area_code, -addressoccupied)
